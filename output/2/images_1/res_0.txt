{"type": "text", "bbox": [1862, 1321, 3377, 4309], "res": [{"text": "Large Pre-Trained Language Models (LLMs) have become", "confidence": 0.9928374886512756, "text_region": [[1938.0, 1324.0], [3371.0, 1324.0], [3371.0, 1377.0], [1938.0, 1377.0]]}, {"text": "ubiquitous in the domain of NLP, achieving impressive per", "confidence": 0.991570234298706, "text_region": [[1872.0, 1386.0], [3368.0, 1393.0], [3367.0, 1446.0], [1871.0, 1439.0]]}, {"text": "formance in many tasks such as machine translation [25], tex1", "confidence": 0.9891828298568726, "text_region": [[1871.0, 1458.0], [3371.0, 1458.0], [3371.0, 1520.0], [1871.0, 1520.0]]}, {"text": "summarization [36] and classification [37]. LLMs follow the", "confidence": 0.9978280663490295, "text_region": [[1868.0, 1530.0], [3371.0, 1526.0], [3371.0, 1589.0], [1868.0, 1592.0]]}, {"text": "Transformer architecture [38] - an encoder to capture inpu", "confidence": 0.980080246925354, "text_region": [[1875.0, 1607.0], [3368.0, 1607.0], [3368.0, 1660.0], [1875.0, 1660.0]]}, {"text": "representation and a decoder to generate output tokens. These", "confidence": 0.9952064752578735, "text_region": [[1875.0, 1679.0], [3371.0, 1679.0], [3371.0, 1732.0], [1875.0, 1732.0]]}, {"text": "LLMs are first pre-trained in an unsupervised manner, on", "confidence": 0.9753044843673706, "text_region": [[1868.0, 1741.0], [3374.0, 1747.0], [3374.0, 1810.0], [1868.0, 1803.0]]}, {"text": "large amounts of text data and then finetuned for downstream", "confidence": 0.9881904125213623, "text_region": [[1875.0, 1819.0], [3368.0, 1819.0], [3368.0, 1872.0], [1875.0, 1872.0]]}, {"text": "tasks. However, certain tasks may not have an abundance of", "confidence": 0.9906932711601257, "text_region": [[1875.0, 1894.0], [3371.0, 1894.0], [3371.0, 1947.0], [1875.0, 1947.0]]}, {"text": "finetuned data available. As such, researchers have evaluatec", "confidence": 0.9891847372055054, "text_region": [[1878.0, 1965.0], [3368.0, 1965.0], [3368.0, 2018.0], [1878.0, 2018.0]]}, {"text": "the ability for LLMs to perform on downstream tasks without", "confidence": 0.987505316734314, "text_region": [[1875.0, 2037.0], [3371.0, 2037.0], [3371.0, 2090.0], [1875.0, 2090.0]]}, {"text": "finetuning. This is achieved via prompt engineering [39] -", "confidence": 0.9909312129020691, "text_region": [[1875.0, 2102.0], [3368.0, 2102.0], [3368.0, 2164.0], [1875.0, 2164.0]]}, {"text": "providing the model with natural language descriptions and", "confidence": 0.9911036491394043, "text_region": [[1868.0, 2177.0], [3374.0, 2174.0], [3374.0, 2236.0], [1868.0, 2239.0]]}, {"text": "demonstrations of the task it is trying to solve before giving the", "confidence": 0.9962810277938843, "text_region": [[1875.0, 2249.0], [3371.0, 2249.0], [3371.0, 2311.0], [1875.0, 2311.0]]}, {"text": "model the target input. This works by leveraging the general-", "confidence": 0.9876450300216675, "text_region": [[1872.0, 2317.0], [3368.0, 2323.0], [3367.0, 2386.0], [1871.0, 2379.0]]}, {"text": "purpose setup of LLMs where the unsupervised pretraining", "confidence": 0.9822310209274292, "text_region": [[1868.0, 2395.0], [3371.0, 2392.0], [3371.0, 2454.0], [1868.0, 2457.0]]}, {"text": "dataset already encompasses many domains of problems/tasks.", "confidence": 0.9986775517463684, "text_region": [[1871.0, 2466.0], [3371.0, 2466.0], [3371.0, 2529.0], [1871.0, 2529.0]]}, {"text": "Using this idea and the exponential growth in LLM size [40],", "confidence": 0.990820050239563, "text_region": [[1871.0, 2535.0], [3371.0, 2535.0], [3371.0, 2597.0], [1871.0, 2597.0]]}, {"text": "impressive performance in many tasks can be achieved even", "confidence": 0.9888894557952881, "text_region": [[1868.0, 2607.0], [3374.0, 2603.0], [3374.0, 2666.0], [1868.0, 2669.0]]}, {"text": "without any finetuning[27].", "confidence": 0.9924945831298828, "text_region": [[1875.0, 2684.0], [2556.0, 2684.0], [2556.0, 2737.0], [1875.0, 2737.0]]}, {"text": "LLMs can be classified into encoder-only, decoder-only and", "confidence": 0.9958745241165161, "text_region": [[1931.0, 2753.0], [3371.0, 2753.0], [3371.0, 2806.0], [1931.0, 2806.0]]}, {"text": "encoder-decoder models based on their architectures. Encoder", "confidence": 0.9923922419548035, "text_region": [[1881.0, 2831.0], [3364.0, 2831.0], [3364.0, 2874.0], [1881.0, 2874.0]]}, {"text": "only models (such as BERT [4i]) contain only the encoder", "confidence": 0.963079035282135, "text_region": [[1878.0, 2899.0], [3371.0, 2899.0], [3371.0, 2952.0], [1878.0, 2952.0]]}, {"text": "component of a Transformer. They are typically designed to", "confidence": 0.9918835163116455, "text_region": [[1875.0, 2971.0], [3374.0, 2971.0], [3374.0, 3024.0], [1875.0, 3024.0]]}, {"text": "learn data representations and are trained using the Masked", "confidence": 0.9774981737136841, "text_region": [[1865.0, 3033.0], [3374.0, 3036.0], [3374.0, 3098.0], [1865.0, 3095.0]]}, {"text": "Language Modeling (MLM) objective - a small percentage", "confidence": 0.984117865562439, "text_region": [[1865.0, 3108.0], [3371.0, 3111.0], [3371.0, 3173.0], [1865.0, 3170.0]]}, {"text": "(e.g., 15%) of tokens in the training data will be replaced by", "confidence": 0.985929548740387, "text_region": [[1878.0, 3185.0], [3368.0, 3185.0], [3368.0, 3238.0], [1878.0, 3238.0]]}, {"text": "masked tokens, and then the models are trained to predict the", "confidence": 0.9972166419029236, "text_region": [[1875.0, 3254.0], [3368.0, 3254.0], [3368.0, 3307.0], [1875.0, 3307.0]]}, {"text": "original values of the masked tokens based on the bidirectional", "confidence": 0.9926385879516602, "text_region": [[1878.0, 3325.0], [3368.0, 3325.0], [3368.0, 3378.0], [1878.0, 3378.0]]}, {"text": "contexts. Decoder-only models (such as GPT-3 [27] and GPT-", "confidence": 0.9792059063911438, "text_region": [[1871.0, 3400.0], [3371.0, 3394.0], [3371.0, 3447.0], [1872.0, 3453.0]]}, {"text": "Neo [42]) are large generative models that use the decoder to", "confidence": 0.9926384091377258, "text_region": [[1871.0, 3469.0], [3374.0, 3469.0], [3374.0, 3531.0], [1871.0, 3531.0]]}, {"text": "predict the next token output given all previous tokens (i.e., leff", "confidence": 0.9890446662902832, "text_region": [[1875.0, 3543.0], [3371.0, 3543.0], [3371.0, 3596.0], [1875.0, 3596.0]]}, {"text": "context or prefix only). To combine the usage of both encoder", "confidence": 0.9887921810150146, "text_region": [[1878.0, 3615.0], [3368.0, 3615.0], [3368.0, 3668.0], [1878.0, 3668.0]]}, {"text": "and decoder, encoder-decoder models (such as T5 [43] anc", "confidence": 0.9884986877441406, "text_region": [[1875.0, 3683.0], [3368.0, 3687.0], [3367.0, 3739.0], [1875.0, 3736.0]]}, {"text": "BART [44]) have also been proposed for sequence-to-sequence", "confidence": 0.9963818192481995, "text_region": [[1868.0, 3752.0], [3371.0, 3758.0], [3371.0, 3820.0], [1868.0, 3814.0]]}, {"text": "tasks where the training objective aims to recover the correci", "confidence": 0.9828139543533325, "text_region": [[1878.0, 3830.0], [3371.0, 3830.0], [3371.0, 3883.0], [1878.0, 3883.0]]}, {"text": "output sequence given the original input (e.g., corrupted to", "confidence": 0.9885324835777283, "text_region": [[1871.0, 3901.0], [3374.0, 3901.0], [3374.0, 3964.0], [1871.0, 3964.0]]}, {"text": "uncorrupted). One such training objective is span prediction", "confidence": 0.9978563189506531, "text_region": [[1871.0, 3970.0], [3374.0, 3967.0], [3374.0, 4029.0], [1872.0, 4032.0]]}, {"text": "tasks, where random spans (multiple tokens) are replaced with", "confidence": 0.9926599264144897, "text_region": [[1875.0, 4041.0], [3371.0, 4041.0], [3371.0, 4104.0], [1875.0, 4104.0]]}, {"text": "artificial span tokens and the model is tasked with recovering", "confidence": 0.9957461953163147, "text_region": [[1878.0, 4116.0], [3371.0, 4116.0], [3371.0, 4169.0], [1878.0, 4169.0]]}, {"text": "the original tokens. For inferencing, one can use the encoder-", "confidence": 0.9955750107765198, "text_region": [[1871.0, 4185.0], [3371.0, 4185.0], [3371.0, 4247.0], [1871.0, 4247.0]]}, {"text": "decoder models to infill text by also adding the artificial", "confidence": 0.9678007960319519, "text_region": [[1872.0, 4253.0], [3371.0, 4256.0], [3371.0, 4308.0], [1871.0, 4306.0]]}], "img_idx": 0}
{"type": "text", "bbox": [1916, 310, 3380, 1084], "res": [{"text": "technique. Among the studied LLMs, the scaling effect", "confidence": 0.9817692041397095, "text_region": [[1927.0, 313.0], [3377.0, 315.0], [3377.0, 366.0], [1927.0, 364.0]]}, {"text": "exists for APR where larger models tend to deliver stronger", "confidence": 0.9956480264663696, "text_region": [[1930.0, 381.0], [3374.0, 387.0], [3374.0, 439.0], [1930.0, 432.0]]}, {"text": "APR results. Also, we show for the first time that suffix", "confidence": 0.995940089225769, "text_region": [[1933.0, 454.0], [3371.0, 455.0], [3371.0, 505.0], [1933.0, 503.0]]}, {"text": "code after the buggy line (adopted in infilling-style APR) is", "confidence": 0.9869987368583679, "text_region": [[1928.0, 526.0], [3377.0, 526.0], [3377.0, 587.0], [1928.0, 587.0]]}, {"text": "important in not only generating more fixes but more patches", "confidence": 0.9830614924430847, "text_region": [[1933.0, 600.0], [3374.0, 600.0], [3374.0, 652.0], [1933.0, 652.0]]}, {"text": "with higher compilation rate. Besides patch generation, the", "confidence": 0.9983547925949097, "text_region": [[1936.0, 673.0], [3369.0, 673.0], [3369.0, 724.0], [1936.0, 724.0]]}, {"text": "LLMs consider correct patches to be more natural than other", "confidence": 0.9879252314567566, "text_region": [[1927.0, 739.0], [3374.0, 741.0], [3374.0, 797.0], [1927.0, 795.0]]}, {"text": "ones, and can even be used for effective patch ranking or", "confidence": 0.9818485379219055, "text_region": [[1934.0, 812.0], [3374.0, 816.0], [3374.0, 868.0], [1934.0, 863.0]]}, {"text": "correctness checking. Lastly, we show that LLM-based APR", "confidence": 0.9897300004959106, "text_region": [[1930.0, 889.0], [3371.0, 884.0], [3371.0, 934.0], [1930.0, 939.0]]}, {"text": "can be further substantially improved via: 1) increasing the", "confidence": 0.9887187480926514, "text_region": [[1930.0, 959.0], [3368.0, 959.0], [3368.0, 1010.0], [1930.0, 1010.0]]}, {"text": "sample size, and 2) incorporating fix template information.", "confidence": 0.9867972135543823, "text_region": [[1928.0, 1028.0], [3366.0, 1027.0], [3366.0, 1076.0], [1928.0, 1078.0]]}], "img_idx": 0}
{"type": "text", "bbox": [288, 311, 1803, 1516], "res": [{"text": "previous learning-based APR, the model (l25M parameters", "confidence": 0.9688431024551392, "text_region": [[296.0, 322.0], [1787.0, 319.0], [1787.0, 366.0], [296.0, 369.0]]}, {"text": "it uses is far smaller than the current state-of-the-art LLMs", "confidence": 0.9944778084754944, "text_region": [[294.0, 383.0], [1794.0, 385.0], [1793.0, 432.0], [294.0, 430.0]]}, {"text": "(Codex: 12B parameters and GPT-3: 175B parameters). Beside", "confidence": 0.9982504844665527, "text_region": [[301.0, 454.0], [1794.0, 457.0], [1793.0, 510.0], [301.0, 507.0]]}, {"text": "AlphaRepair, researchers have also directly leveraged Codex", "confidence": 0.9906740188598633, "text_region": [[305.0, 531.0], [1789.0, 529.0], [1789.0, 578.0], [305.0, 579.0]]}, {"text": "for generative APR [31], [32], i.e., generating the fixes based", "confidence": 0.991426944732666, "text_region": [[296.0, 598.0], [1797.0, 600.0], [1797.0, 651.0], [296.0, 650.0]]}, {"text": "on the context before bugs (i.e., prefix only). However, these", "confidence": 0.9940875172615051, "text_region": [[296.0, 669.0], [1797.0, 667.0], [1797.0, 725.0], [296.0, 727.0]]}, {"text": "studies mostly focus on Codex and are only evaluated on a", "confidence": 0.9904111623764038, "text_region": [[301.0, 742.0], [1798.0, 746.0], [1798.0, 797.0], [301.0, 794.0]]}, {"text": "small dataset with 40 bugs on simple programming tasks.", "confidence": 0.9962137937545776, "text_region": [[298.0, 813.0], [1710.0, 815.0], [1710.0, 874.0], [297.0, 873.0]]}, {"text": "Current state-of-the-art LLMs [28], [33] have also included", "confidence": 0.9950775504112244, "text_region": [[356.0, 898.0], [1795.0, 898.0], [1795.0, 950.0], [356.0, 950.0]]}, {"text": "evaluation for code related tasks such as code completion [28]", "confidence": 0.9931057095527649, "text_region": [[301.0, 968.0], [1792.0, 973.0], [1792.0, 1020.0], [301.0, 1015.0]]}, {"text": "docstring generation[34]and variable/type prediction[34].", "confidence": 0.9689368009567261, "text_region": [[301.0, 1044.0], [1794.0, 1044.0], [1794.0, 1096.0], [301.0, 1096.0]]}, {"text": "However, these evaluations still mainly focus on NLP metrics", "confidence": 0.9916061758995056, "text_region": [[298.0, 1111.0], [1797.0, 1113.0], [1797.0, 1166.0], [297.0, 1165.0]]}, {"text": "such as BLEU score [35] which do not accurately measure the", "confidence": 0.9931422472000122, "text_region": [[299.0, 1185.0], [1789.0, 1187.0], [1789.0, 1235.0], [299.0, 1234.0]]}, {"text": "functional or semantic correctness of the generated code. Fur-", "confidence": 0.988348126411438, "text_region": [[296.0, 1256.0], [1790.0, 1256.0], [1790.0, 1307.0], [296.0, 1307.0]]}, {"text": "thermore, the datasets consist of hand-curated code problems", "confidence": 0.9957832098007202, "text_region": [[299.0, 1329.0], [1794.0, 1331.0], [1793.0, 1378.0], [299.0, 1376.0]]}, {"text": "which do not accurately reflect the type of projects developers", "confidence": 0.9910929203033447, "text_region": [[299.0, 1398.0], [1795.0, 1403.0], [1795.0, 1456.0], [299.0, 1452.0]]}, {"text": "york on in the real world", "confidence": 0.9646859765052795, "text_region": [[316.0, 1474.0], [932.0, 1474.0], [932.0, 1510.0], [316.0, 1510.0]]}], "img_idx": 0}
{"type": "text", "bbox": [287, 2998, 1805, 4309], "res": [{"text": "cent advances in LLMs and a crucial software engineering", "confidence": 0.9950490593910217, "text_region": [[350.0, 3034.0], [1794.0, 3041.0], [1794.0, 3094.0], [350.0, 3088.0]]}, {"text": "problem - APR. This paper not only", "confidence": 0.9542346000671387, "text_region": [[347.0, 3104.0], [1351.0, 3107.0], [1351.0, 3165.0], [347.0, 3162.0]]}, {"text": "ydemonstratesthe", "confidence": 0.9838294386863708, "text_region": [[1323.0, 3115.0], [1794.0, 3111.0], [1794.0, 3155.0], [1323.0, 3159.0]]}, {"text": "potential and future for directly leveraging LLMs for solving", "confidence": 0.9832590222358704, "text_region": [[344.0, 3181.0], [1796.0, 3182.0], [1795.0, 3236.0], [344.0, 3234.0]]}, {"text": "the important APR problem, but also provides a realistic", "confidence": 0.974563479423523, "text_region": [[350.0, 3253.0], [1796.0, 3253.0], [1796.0, 3305.0], [350.0, 3305.0]]}, {"text": "evaluation scenario for the recent LLMs, which were mainly", "confidence": 0.9970196485519409, "text_region": [[352.0, 3324.0], [1791.0, 3326.0], [1791.0, 3375.0], [352.0, 3373.0]]}, {"text": "evaluated on simple/synthetic coding problems rather than", "confidence": 0.9817315936088562, "text_region": [[350.0, 3398.0], [1799.0, 3398.0], [1799.0, 3450.0], [350.0, 3450.0]]}, {"text": "real-world systems as studied in the APR area.", "confidence": 0.9954060316085815, "text_region": [[350.0, 3471.0], [1498.0, 3471.0], [1498.0, 3518.0], [350.0, 3518.0]]}, {"text": "Study. We conduct extensive evaluations using 9 different", "confidence": 0.9937855005264282, "text_region": [[342.0, 3538.0], [1800.0, 3540.0], [1800.0, 3595.0], [342.0, 3594.0]]}, {"text": "recent LLMs on 5 different repair datasets across 3 different", "confidence": 0.9833731651306152, "text_region": [[347.0, 3611.0], [1799.0, 3609.0], [1799.0, 3661.0], [347.0, 3663.0]]}, {"text": "programming languages (Java, Python, and C). We compare", "confidence": 0.9915778636932373, "text_region": [[349.0, 3683.0], [1799.0, 3680.0], [1799.0, 3739.0], [349.0, 3742.0]]}, {"text": "the LLMs against each other using the 3 repair settings", "confidence": 0.9804704785346985, "text_region": [[346.0, 3750.0], [1799.0, 3756.0], [1798.0, 3814.0], [345.0, 3808.0]]}, {"text": "we designed. Using the popular repair datasets, we further", "confidence": 0.988860547542572, "text_region": [[352.0, 3828.0], [1799.0, 3828.0], [1799.0, 3880.0], [352.0, 3880.0]]}, {"text": "compare the LLMs with state-of-the-art APR tools.", "confidence": 0.9904274344444275, "text_region": [[353.0, 3899.0], [1609.0, 3895.0], [1609.0, 3947.0], [354.0, 3951.0]]}, {"text": " Practical Guidelines. Our study shows for the first time", "confidence": 0.9743251204490662, "text_region": [[330.0, 3970.0], [1797.0, 3972.0], [1797.0, 4019.0], [330.0, 4017.0]]}, {"text": "thatdirectly", "confidence": 0.9992830157279968, "text_region": [[351.0, 4043.0], [664.0, 4046.0], [663.0, 4093.0], [350.0, 4090.0]]}, {"text": "1applying state-of-the-art LLMs can already", "confidence": 0.9769138097763062, "text_region": [[643.0, 4044.0], [1792.0, 4039.0], [1792.0, 4091.0], [643.0, 4096.0]]}, {"text": "substantially outperform all existing APR tools on the", "confidence": 0.9729869365692139, "text_region": [[352.0, 4114.0], [1792.0, 4114.0], [1792.0, 4166.0], [352.0, 4166.0]]}, {"text": "widely studied Defects4J 1.2 dataset (and other ones), e.g.", "confidence": 0.9930394291877747, "text_region": [[354.0, 4183.0], [1796.0, 4188.0], [1795.0, 4241.0], [353.0, 4236.0]]}, {"text": "Codex can fix 32 more bugs than the existing best APR", "confidence": 0.9877620935440063, "text_region": [[355.0, 4257.0], [1794.0, 4257.0], [1794.0, 4300.0], [355.0, 4300.0]]}], "img_idx": 0}
{"type": "text", "bbox": [289, 1568, 1805, 2899], "res": [{"text": "Our Work. We present the first extensive evaluation of recent", "confidence": 0.9715521931648254, "text_region": [[299.0, 1571.0], [1800.0, 1573.0], [1800.0, 1622.0], [298.0, 1621.0]]}, {"text": "LLMs for fixing real-world projects. We designed 3 different", "confidence": 0.9811304211616516, "text_region": [[297.0, 1638.0], [1797.0, 1640.0], [1797.0, 1693.0], [297.0, 1691.0]]}, {"text": "APR experimental settings: 1) complete function generation 2)", "confidence": 0.9849200248718262, "text_region": [[300.0, 1710.0], [1792.0, 1712.0], [1792.0, 1765.0], [300.0, 1763.0]]}, {"text": "correct code infilling and 3) single line generation to showcase", "confidence": 0.9746047258377075, "text_region": [[300.0, 1786.0], [1795.0, 1784.0], [1796.0, 1834.0], [300.0, 1835.0]]}, {"text": "the different ways LLMs can be applied for APR. In our", "confidence": 0.9800921678543091, "text_region": [[295.0, 1853.0], [1799.0, 1854.0], [1799.0, 1909.0], [295.0, 1907.0]]}, {"text": "study, we include both popular types of LLM architectures", "confidence": 0.9928819537162781, "text_region": [[298.0, 1923.0], [1799.0, 1922.0], [1799.0, 1981.0], [299.0, 1982.0]]}, {"text": "(generative and infilling models) to show the advantages and", "confidence": 0.9745075702667236, "text_region": [[306.0, 2002.0], [1794.0, 1998.0], [1794.0, 2046.0], [306.0, 2050.0]]}, {"text": "flaws of using each type for APR. We include models with", "confidence": 0.9818417429924011, "text_region": [[297.0, 2070.0], [1797.0, 2070.0], [1797.0, 2125.0], [297.0, 2125.0]]}, {"text": "a wide range of different parameter sizes, spanning from 125", "confidence": 0.9911375045776367, "text_region": [[297.0, 2141.0], [1794.0, 2141.0], [1794.0, 2194.0], [297.0, 2194.0]]}, {"text": "million to 20 billion. We evaluate not only the improvement in", "confidence": 0.9874505400657654, "text_region": [[300.0, 2211.0], [1792.0, 2214.0], [1792.0, 2269.0], [300.0, 2265.0]]}, {"text": "repair effectiveness but also the trade-off with respect to speed", "confidence": 0.97677081823349, "text_region": [[297.0, 2280.0], [1799.0, 2283.0], [1799.0, 2344.0], [297.0, 2341.0]]}, {"text": "when increasing the model size. In total, we use 5 different", "confidence": 0.9865182638168335, "text_region": [[302.0, 2357.0], [1802.0, 2357.0], [1802.0, 2409.0], [302.0, 2409.0]]}, {"text": "repair datasets containing real open-source bugs and developer", "confidence": 0.996616005897522, "text_region": [[297.0, 2429.0], [1797.0, 2429.0], [1797.0, 2481.0], [297.0, 2481.0]]}, {"text": "written tests across 3 programming languages to evaluate APR", "confidence": 0.9865882992744446, "text_region": [[297.0, 2497.0], [1797.0, 2496.0], [1797.0, 2555.0], [297.0, 2557.0]]}, {"text": "under realistic settings. Compared with existing applications of", "confidence": 0.9848540425300598, "text_region": [[297.0, 2568.0], [1800.0, 2571.0], [1800.0, 2630.0], [297.0, 2627.0]]}, {"text": "LLMs for APR [26], [31], [32], our study is the first to include", "confidence": 0.991346001625061, "text_region": [[297.0, 2643.0], [1797.0, 2643.0], [1797.0, 2696.0], [297.0, 2696.0]]}, {"text": "state-of-the-art LLMs for both infilling-style and generative", "confidence": 0.9891230463981628, "text_region": [[294.0, 2710.0], [1794.0, 2717.0], [1794.0, 2774.0], [294.0, 2768.0]]}, {"text": "APR on various datasets and programming", "confidence": 0.9789292812347412, "text_region": [[300.0, 2784.0], [1425.0, 2792.0], [1424.0, 2842.0], [300.0, 2833.0]]}, {"text": "languages. To", "confidence": 0.9767891764640808, "text_region": [[1426.0, 2790.0], [1796.0, 2790.0], [1796.0, 2840.0], [1426.0, 2840.0]]}, {"text": "2", "confidence": 0.03777150809764862, "text_region": [[1409.0, 2808.0], [1435.0, 2808.0], [1435.0, 2829.0], [1409.0, 2829.0]]}, {"text": "7Pt510", "confidence": 0.47980818152427673, "text_region": [[507.0, 2867.0], [673.0, 2867.0], [673.0, 2883.0], [507.0, 2883.0]]}, {"text": "ozeatbe+o", "confidence": 0.5453886389732361, "text_region": [[905.0, 2867.0], [1183.0, 2867.0], [1183.0, 2883.0], [905.0, 2883.0]]}], "img_idx": 0}
{"type": "text", "bbox": [1873, 1138, 3122, 1281], "res": [{"text": "II. BACKGROUND AND RELATED WORI", "confidence": 0.949478805065155, "text_region": [[2114.0, 1144.0], [3116.0, 1147.0], [3115.0, 1187.0], [2114.0, 1184.0]]}], "img_idx": 0}
{"type": "title", "bbox": [2110, 1139, 3139, 1282], "res": [{"text": "II. BACKGROUND AND RELATED WORK", "confidence": 0.9429879188537598, "text_region": [[2120.0, 1147.0], [3132.0, 1149.0], [3131.0, 1184.0], [2120.0, 1181.0]]}, {"text": "Pre-Trained LanguageModel", "confidence": 0.9871066212654114, "text_region": [[2117.0, 1227.0], [2838.0, 1230.0], [2838.0, 1274.0], [2116.0, 1272.0]]}], "img_idx": 0}
