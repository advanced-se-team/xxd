{"type": "text", "bbox": [294, 883, 1799, 4310], "res": [{"text": "Autor", "confidence": 0.906893253326416, "text_region": [[374.0, 901.0], [489.0, 901.0], [489.0, 926.0], [374.0, 926.0]]}, {"text": "e used to generate", "confidence": 0.9314970374107361, "text_region": [[1376.0, 897.0], [1785.0, 897.0], [1785.0, 933.0], [1376.0, 933.0]]}, {"text": "oatched code given the original code and the corresponding", "confidence": 0.9868818521499634, "text_region": [[308.0, 962.0], [1788.0, 962.0], [1788.0, 1012.0], [308.0, 1012.0]]}, {"text": "buggy location. Each patch generated by the APR tool is", "confidence": 0.977915346622467, "text_region": [[301.0, 1037.0], [1792.0, 1026.0], [1792.0, 1076.0], [301.0, 1087.0]]}, {"text": "validated against the test suite. Plausible patches are one", "confidence": 0.982381284236908, "text_region": [[308.0, 1104.0], [1781.0, 1104.0], [1781.0, 1154.0], [308.0, 1154.0]]}, {"text": "vhich pass the entire suite. Correct patches are plausible", "confidence": 0.9799099564552307, "text_region": [[308.0, 1176.0], [1788.0, 1176.0], [1788.0, 1226.0], [308.0, 1226.0]]}, {"text": "Traditional APR tools", "confidence": 0.9611647725105286, "text_region": [[370.0, 1319.0], [952.0, 1319.0], [952.0, 1365.0], [370.0, 1365.0]]}, {"text": "classified as heuristic", "confidence": 0.980796217918396, "text_region": [[1206.0, 1319.0], [1781.0, 1319.0], [1781.0, 1365.0], [1206.0, 1365.0]]}, {"text": "be", "confidence": 0.9548768401145935, "text_region": [[1119.0, 1329.0], [1162.0, 1329.0], [1162.0, 1354.0], [1119.0, 1354.0]]}, {"text": "[1o]-[12\u300dand template", "confidence": 0.9375274181365967, "text_region": [[1112.0, 1393.0], [1777.0, 1393.0], [1777.0, 1440.0], [1112.0, 1440.0]]}, {"text": "based [13]-[16], [19]. Traditionally, template-based APR tools", "confidence": 0.996812641620636, "text_region": [[301.0, 1461.0], [1788.0, 1458.0], [1788.0, 1508.0], [301.0, 1511.0]]}, {"text": "achieve the best performance", "confidence": 0.9806474447250366, "text_region": [[305.0, 1540.0], [1018.0, 1540.0], [1018.0, 1586.0], [305.0, 1586.0]]}, {"text": "vhere each template is hand", "confidence": 0.975764811038971, "text_region": [[1079.0, 1536.0], [1785.0, 1536.0], [1785.0, 1583.0], [1079.0, 1583.0]]}, {"text": "crafted by human experts  designed", "confidence": 0.9723819494247437, "text_region": [[308.0, 1611.0], [1188.0, 1611.0], [1188.0, 1658.0], [308.0, 1658.0]]}, {"text": " to provide a fix for ", "confidence": 0.9460687637329102, "text_region": [[1191.0, 1608.0], [1774.0, 1608.0], [1774.0, 1654.0], [1191.0, 1654.0]]}, {"text": "specific type of bug. However, these template-based APR tools", "confidence": 0.9871887564659119, "text_region": [[301.0, 1676.0], [1792.0, 1668.0], [1792.0, 1729.0], [301.0, 1736.0]]}, {"text": "can only fix the bug types that are part of the templates. As", "confidence": 0.9860153794288635, "text_region": [[298.0, 1747.0], [1795.0, 1740.0], [1796.0, 1800.0], [298.0, 1808.0]]}, {"text": "a result, researchers employe", "confidence": 0.9626598358154297, "text_region": [[305.0, 1829.0], [1010.0, 1825.0], [1010.0, 1865.0], [305.0, 1868.0]]}, {"text": "learning-based APR tools to", "confidence": 0.9459230899810791, "text_region": [[1083.0, 1822.0], [1792.0, 1822.0], [1792.0, 1868.0], [1083.0, 1868.0]]}, {"text": "earning-based APR tools", "confidence": 0.9613836407661438, "text_region": [[1191.0, 1893.0], [1785.0, 1893.0], [1785.0, 1940.0], [1191.0, 1940.0]]}, {"text": "generate more expressive patc", "confidence": 0.969232976436615, "text_region": [[301.0, 1904.0], [1021.0, 1897.0], [1021.0, 1943.0], [302.0, 1951.0]]}, {"text": "[25] which require specific bug", "confidence": 0.9718472957611084, "text_region": [[1043.0, 2029.0], [1788.0, 2040.0], [1788.0, 2090.0], [1043.0, 2079.0]]}, {"text": "are based on NMT techniques", "confidence": 0.9460030794143677, "text_region": [[301.0, 2043.0], [1061.0, 2040.0], [1061.0, 2079.0], [301.0, 2083.0]]}, {"text": "given the buggy line. Due to this reliance on the bug-fixing", "confidence": 0.9829002618789673, "text_region": [[298.0, 2179.0], [1792.0, 2175.0], [1792.0, 2236.0], [298.0, 2240.0]]}, {"text": "data, these learning-based tools are still limited in terms of the", "confidence": 0.9884481430053711, "text_region": [[301.0, 2254.0], [1784.0, 2247.0], [1785.0, 2297.0], [301.0, 2304.0]]}, {"text": "APR under a zero-shot setting by", "confidence": 0.9844799041748047, "text_region": [[1003.0, 2389.0], [1788.0, 2397.0], [1788.0, 2447.0], [1003.0, 2439.0]]}, {"text": "ills the original buggy line with masked tokens and uses", "confidence": 0.976883053779602, "text_region": [[308.0, 2539.0], [1788.0, 2539.0], [1788.0, 2589.0], [308.0, 2589.0]]}, {"text": "CodeBERT to replace the masked tokens with correct code", "confidence": 0.990755021572113, "text_region": [[308.0, 2611.0], [1788.0, 2611.0], [1788.0, 2657.0], [308.0, 2657.0]]}, {"text": "style) APR. While AlphaRepair is able to achieve state-of", "confidence": 0.9741876125335693, "text_region": [[301.0, 2754.0], [1788.0, 2750.0], [1788.0, 2800.0], [301.0, 2804.0]]}, {"text": "the-art results, CodeBERT is considerably smaller than the", "confidence": 0.980963945388794, "text_region": [[301.0, 2825.0], [1788.0, 2821.0], [1788.0, 2871.0], [301.0, 2875.0]]}, {"text": "newest LLMs. Additionally, AlphaRepair is designed for the", "confidence": 0.9843302965164185, "text_region": [[305.0, 2896.0], [1788.0, 2896.0], [1788.0, 2946.0], [305.0, 2946.0]]}, {"text": "computed by fault localization techniques [46]).", "confidence": 0.9938445091247559, "text_region": [[294.0, 3036.0], [1466.0, 3032.0], [1466.0, 3093.0], [294.0, 3096.0]]}, {"text": "Recent work [31], [32] has also looked into directly apply", "confidence": 0.9960574507713318, "text_region": [[356.0, 3103.0], [1785.0, 3111.0], [1784.0, 3171.0], [355.0, 3164.0]]}, {"text": "ing LLMs for APR. Prenner et al. [32] conducted a small-scale", "confidence": 0.9852561950683594, "text_region": [[301.0, 3186.0], [1788.0, 3182.0], [1788.0, 3232.0], [301.0, 3236.0]]}, {"text": "evaluation for the Codex model on a simple dataset containing", "confidence": 0.968273937702179, "text_region": [[305.0, 3257.0], [1788.0, 3257.0], [1788.0, 3307.0], [305.0, 3307.0]]}, {"text": "both Java and Python versions of buggy algorithm imple", "confidence": 0.9786862134933472, "text_region": [[301.0, 3328.0], [1784.0, 3325.0], [1785.0, 3375.0], [301.0, 3378.0]]}, {"text": "mentations. Codex is given the buggy function and by using", "confidence": 0.9878666996955872, "text_region": [[305.0, 3400.0], [1788.0, 3400.0], [1788.0, 3450.0], [305.0, 3450.0]]}, {"text": "ixed function. The results show that Codex is competitive", "confidence": 0.9845795631408691, "text_region": [[305.0, 3539.0], [1785.0, 3539.0], [1785.0, 3589.0], [305.0, 3589.0]]}, {"text": "with state-of-the-art learning-based APR tools in Python bui", "confidence": 0.9757865071296692, "text_region": [[305.0, 3614.0], [1792.0, 3614.0], [1792.0, 3664.0], [305.0, 3664.0]]}, {"text": "worse in Java. In contrast, we show that by using our repaii", "confidence": 0.9815523624420166, "text_region": [[298.0, 3678.0], [1792.0, 3685.0], [1792.0, 3746.0], [297.0, 3739.0]]}, {"text": "settings, LLMs are able to outperform state-of-the-art APR", "confidence": 0.9889239072799683, "text_region": [[305.0, 3760.0], [1788.0, 3753.0], [1788.0, 3803.0], [305.0, 3810.0]]}, {"text": "tools on both Java and Python. Kolak et al. [3l] also usec", "confidence": 0.95673668384552, "text_region": [[305.0, 3832.0], [1785.0, 3832.0], [1785.0, 3878.0], [305.0, 3878.0]]}, {"text": "Codex along with 2 smaller LLMs and evaluated their ability", "confidence": 0.9934774041175842, "text_region": [[301.0, 3903.0], [1785.0, 3903.0], [1785.0, 3953.0], [301.0, 3953.0]]}, {"text": "to generate the correct patch line when given the code prefix", "confidence": 0.9803493618965149, "text_region": [[298.0, 3974.0], [1788.0, 3974.0], [1788.0, 4024.0], [298.0, 4024.0]]}, {"text": "on the same dataset as the previous work [32]. The evaluation", "confidence": 0.9980573058128357, "text_region": [[298.0, 4042.0], [1792.0, 4042.0], [1792.0, 4092.0], [298.0, 4092.0]]}, {"text": "demonstrated the scaling effect of LLMs where the repair", "confidence": 0.9764119982719421, "text_region": [[298.0, 4110.0], [1795.0, 4110.0], [1795.0, 4171.0], [298.0, 4171.0]]}, {"text": "cesults can be improved by using larger models. Interestingly", "confidence": 0.996282696723938, "text_region": [[305.0, 4189.0], [1785.0, 4189.0], [1785.0, 4239.0], [305.0, 4239.0]]}, {"text": "1 for patch ranking while", "confidence": 0.956806480884552, "text_region": [[1130.0, 4253.0], [1788.0, 4257.0], [1788.0, 4307.0], [1130.0, 4303.0]]}, {"text": "sum", "confidence": 0.904802143573761, "text_region": [[837.0, 4271.0], [938.0, 4271.0], [938.0, 4296.0], [837.0, 4296.0]]}], "img_idx": 0}
{"type": "text", "bbox": [1874, 944, 3381, 1935], "res": [{"text": "AlphaRepair leverages mean entropy (i.e., both favors more", "confidence": 0.9721776247024536, "text_region": [[1885.0, 953.0], [3375.0, 950.0], [3375.0, 1001.0], [1885.0, 1004.0]]}, {"text": "natural [47] patches). Thus, we also perform a study of", "confidence": 0.9794802665710449, "text_region": [[1882.0, 1020.0], [3378.0, 1020.0], [3378.0, 1073.0], [1882.0, 1073.0]]}, {"text": "everaging various recent LLMs for computing both entropies", "confidence": 0.9816493988037109, "text_region": [[1883.0, 1090.0], [3372.0, 1090.0], [3372.0, 1147.0], [1883.0, 1147.0]]}, {"text": "for patch ranking on real-world systems. In addition, to the", "confidence": 0.9816932678222656, "text_region": [[1879.0, 1161.0], [3373.0, 1158.0], [3373.0, 1215.0], [1879.0, 1218.0]]}, {"text": "best of our knowledge, we are the first to study LLMs or", "confidence": 0.9886919260025024, "text_region": [[1879.0, 1230.0], [3376.0, 1235.0], [3376.0, 1286.0], [1879.0, 1282.0]]}, {"text": "entropies for patch correctness checking (i.e., distinguishing", "confidence": 0.9809660315513611, "text_region": [[1882.0, 1299.0], [3378.0, 1302.0], [3378.0, 1364.0], [1882.0, 1360.0]]}, {"text": "correct patches from plausible ones).", "confidence": 0.9965971112251282, "text_region": [[1885.0, 1378.0], [2773.0, 1378.0], [2773.0, 1430.0], [1885.0, 1430.0]]}, {"text": "Overall, the 2 prior studies [31], [32] are done on a small", "confidence": 0.9770688414573669, "text_region": [[1934.0, 1446.0], [3375.0, 1443.0], [3375.0, 1495.0], [1934.0, 1498.0]]}, {"text": "dataset with synthetic bugs using only a small number of", "confidence": 0.9791646599769592, "text_region": [[1877.0, 1517.0], [3380.0, 1512.0], [3380.0, 1574.0], [1877.0, 1579.0]]}, {"text": "LLMs. Moreover, the input and repair setting being used in the", "confidence": 0.9975762963294983, "text_region": [[1883.0, 1590.0], [3368.0, 1590.0], [3368.0, 1647.0], [1883.0, 1647.0]]}, {"text": "studies are also limited, e.g., only considered generative APR", "confidence": 0.9928911924362183, "text_region": [[1885.0, 1662.0], [3367.0, 1662.0], [3367.0, 1714.0], [1885.0, 1714.0]]}, {"text": "n this paper, we present an extensive study of applying various", "confidence": 0.9930097460746765, "text_region": [[1887.0, 1735.0], [3375.0, 1735.0], [3375.0, 1788.0], [1887.0, 1788.0]]}, {"text": "state-of-the-art LLMs for both infilling-style and generative", "confidence": 0.9838157296180725, "text_region": [[1879.0, 1803.0], [3372.0, 1807.0], [3372.0, 1858.0], [1879.0, 1854.0]]}, {"text": "APR on diverse repair datasets across programming languages", "confidence": 0.9849141240119934, "text_region": [[1880.0, 1875.0], [3365.0, 1878.0], [3365.0, 1926.0], [1880.0, 1923.0]]}], "img_idx": 0}
{"type": "text", "bbox": [289, 312, 1804, 721], "res": [{"text": "span token in place. Recently, researchers have also combined", "confidence": 0.9875522255897522, "text_region": [[297.0, 322.0], [1796.0, 318.0], [1796.0, 366.0], [297.0, 370.0]]}, {"text": "MLM with generative models to perform both bidirectional", "confidence": 0.9795419573783875, "text_region": [[300.0, 387.0], [1798.0, 384.0], [1798.0, 433.0], [300.0, 437.0]]}, {"text": "and autoregressive text generation or infilling [45]. In our APR", "confidence": 0.9834941029548645, "text_region": [[303.0, 459.0], [1793.0, 457.0], [1793.0, 507.0], [303.0, 509.0]]}, {"text": "scenario, all types of LLMs can potentially be leveraged for", "confidence": 0.9866834282875061, "text_region": [[303.0, 531.0], [1795.0, 531.0], [1795.0, 579.0], [303.0, 579.0]]}, {"text": "generative or infilling-style APR, and we select 9 state-of-the-", "confidence": 0.9884088635444641, "text_region": [[297.0, 601.0], [1793.0, 596.0], [1793.0, 646.0], [297.0, 651.0]]}, {"text": "art LLMs for our study (detailed in Section II-A).", "confidence": 0.9881009459495544, "text_region": [[299.0, 670.0], [1539.0, 673.0], [1539.0, 716.0], [298.0, 713.0]]}], "img_idx": 0}
{"type": "text", "bbox": [1867, 2666, 3378, 4310], "res": [{"text": "We begin by describing the different LLMs we use for", "confidence": 0.9730907082557678, "text_region": [[1933.0, 2676.0], [3371.0, 2678.0], [3371.0, 2735.0], [1933.0, 2733.0]]}, {"text": "evaluation. Our selection process starts with the list of populai", "confidence": 0.9908976554870605, "text_region": [[1881.0, 2752.0], [3366.0, 2755.0], [3366.0, 2806.0], [1880.0, 2803.0]]}, {"text": "models hosted on the Hugging Face [48] - an open-source", "confidence": 0.9765785336494446, "text_region": [[1874.0, 2818.0], [3375.0, 2827.0], [3374.0, 2884.0], [1874.0, 2875.0]]}, {"text": "platform to host and deploy large models. We sort the list", "confidence": 0.9818824529647827, "text_region": [[1877.0, 2897.0], [3373.0, 2897.0], [3373.0, 2949.0], [1877.0, 2949.0]]}, {"text": "of models based on popularity (#downloads this month) and", "confidence": 0.976386308670044, "text_region": [[1879.0, 2966.0], [3373.0, 2967.0], [3373.0, 3021.0], [1879.0, 3019.0]]}, {"text": "select the LLMs which contain code as training data. Fur", "confidence": 0.9709354639053345, "text_region": [[1876.0, 3038.0], [3366.0, 3041.0], [3366.0, 3092.0], [1875.0, 3089.0]]}, {"text": "thermore, we also pick models from different organizations", "confidence": 0.9667322635650635, "text_region": [[1874.0, 3111.0], [3375.0, 3111.0], [3375.0, 3168.0], [1874.0, 3168.0]]}, {"text": "and types (described below) to obtain a diverse set of models", "confidence": 0.9908320307731628, "text_region": [[1880.0, 3185.0], [3364.0, 3183.0], [3365.0, 3231.0], [1881.0, 3233.0]]}, {"text": "Along with the open-source models, we also use the closed-", "confidence": 0.9838765263557434, "text_region": [[1879.0, 3255.0], [3370.0, 3253.0], [3370.0, 3306.0], [1879.0, 3308.0]]}, {"text": "source Codex model [28](accessible only via API) since it", "confidence": 0.958570659160614, "text_region": [[1877.0, 3329.0], [3375.0, 3329.0], [3375.0, 3380.0], [1877.0, 3380.0]]}, {"text": "has shown to achieve impressive performance on code related", "confidence": 0.9845896363258362, "text_region": [[1872.0, 3396.0], [3373.0, 3397.0], [3373.0, 3450.0], [1872.0, 3449.0]]}, {"text": "tasks. In total, we use 9 different LLMs for our experiment.", "confidence": 0.9920217394828796, "text_region": [[1876.0, 3466.0], [3349.0, 3473.0], [3349.0, 3524.0], [1875.0, 3517.0]]}, {"text": "Our chosen LLMs range from 125M to 20B in parameter", "confidence": 0.9742436408996582, "text_region": [[1933.0, 3538.0], [3370.0, 3539.0], [3370.0, 3591.0], [1933.0, 3589.0]]}, {"text": "size. Table I presents the LLM overview. Column Model is", "confidence": 0.9838805198669434, "text_region": [[1877.0, 3611.0], [3375.0, 3611.0], [3375.0, 3663.0], [1877.0, 3663.0]]}, {"text": "the model name, #Parameters presents the number of model", "confidence": 0.997148334980011, "text_region": [[1877.0, 3683.0], [3370.0, 3683.0], [3370.0, 3735.0], [1877.0, 3735.0]]}, {"text": "parameters, Training Dataset indicates the dataset used for", "confidence": 0.985832154750824, "text_region": [[1875.0, 3757.0], [3373.0, 3752.0], [3373.0, 3803.0], [1876.0, 3808.0]]}, {"text": "pre-training (N.R. is not released), and Type refers to the type", "confidence": 0.995485246181488, "text_region": [[1875.0, 3827.0], [3370.0, 3829.0], [3370.0, 3882.0], [1875.0, 3880.0]]}, {"text": "of APR the model can perform (infilling or generative).", "confidence": 0.9862521290779114, "text_region": [[1872.0, 3896.0], [3241.0, 3899.0], [3241.0, 3956.0], [1872.0, 3952.0]]}, {"text": "1) Generative Models:", "confidence": 0.9408890008926392, "text_region": [[1935.0, 3964.0], [2491.0, 3968.0], [2491.0, 4019.0], [1934.0, 4015.0]]}, {"text": " GPT-Ne0 [42], GPT-J [49], GPT-Ne0X [50] All three", "confidence": 0.9313476085662842, "text_region": [[1907.0, 4041.0], [3371.0, 4041.0], [3371.0, 4093.0], [1907.0, 4093.0]]}, {"text": "models are open-source implementations of the GPT-3 trans-", "confidence": 0.9826180934906006, "text_region": [[1926.0, 4115.0], [3363.0, 4113.0], [3363.0, 4166.0], [1926.0, 4168.0]]}, {"text": "former architecture [27]. In our experiments, we use GPT", "confidence": 0.9969258904457092, "text_region": [[1924.0, 4185.0], [3366.0, 4187.0], [3366.0, 4238.0], [1924.0, 4236.0]]}, {"text": "Neo models with 125M. 1.3B and 2.7B parameters. GPT-J", "confidence": 0.9745317697525024, "text_region": [[1931.0, 4259.0], [3368.0, 4259.0], [3368.0, 4300.0], [1931.0, 4300.0]]}], "img_idx": 0}
{"type": "text", "bbox": [1875, 2061, 3380, 2562], "res": [{"text": "In this section we describe the LLMs selected for evaluatior", "confidence": 0.9880091547966003, "text_region": [[1933.0, 2067.0], [3367.0, 2070.0], [3367.0, 2113.0], [1933.0, 2110.0]]}, {"text": "and introduce 3 different APR generation settings we use to", "confidence": 0.9742933511734009, "text_region": [[1881.0, 2133.0], [3374.0, 2138.0], [3374.0, 2191.0], [1881.0, 2186.0]]}, {"text": "evaluate each LLM. These settings are designed to showcase", "confidence": 0.983184814453125, "text_region": [[1883.0, 2203.0], [3375.0, 2207.0], [3375.0, 2263.0], [1883.0, 2260.0]]}, {"text": "the different practical ways we can directly use LLMs for", "confidence": 0.9778378009796143, "text_region": [[1881.0, 2279.0], [3374.0, 2279.0], [3374.0, 2332.0], [1881.0, 2332.0]]}, {"text": "APR and highlight advantages and differences of the studied", "confidence": 0.988537073135376, "text_region": [[1881.0, 2352.0], [3375.0, 2347.0], [3375.0, 2401.0], [1881.0, 2405.0]]}, {"text": "LLM types. Also, we detail the patch ranking strategy of using", "confidence": 0.9961643815040588, "text_region": [[1885.0, 2421.0], [3369.0, 2426.0], [3369.0, 2478.0], [1884.0, 2473.0]]}, {"text": "entropy to prioritize patches that are more likely to be correct", "confidence": 0.9868868589401245, "text_region": [[1886.0, 2496.0], [3371.0, 2496.0], [3371.0, 2548.0], [1886.0, 2548.0]]}], "img_idx": 0}
{"type": "title", "bbox": [291, 788, 1050, 838], "res": [{"text": "B. Automated", "confidence": 0.9780567288398743, "text_region": [[292.0, 790.0], [640.0, 790.0], [640.0, 836.0], [292.0, 836.0]]}, {"text": "Program", "confidence": 0.9988760352134705, "text_region": [[635.0, 796.0], [865.0, 800.0], [864.0, 833.0], [634.0, 828.0]]}, {"text": "Repair", "confidence": 0.9982374310493469, "text_region": [[885.0, 794.0], [1047.0, 797.0], [1046.0, 836.0], [884.0, 833.0]]}], "img_idx": 0}
{"type": "title", "bbox": [2426, 1976, 2819, 2019], "res": [{"text": "PPRCOAC", "confidence": 0.8344958424568176, "text_region": [[2591.0, 1988.0], [2768.0, 1992.0], [2768.0, 2011.0], [2591.0, 2007.0]]}], "img_idx": 0}
{"type": "title", "bbox": [1866, 2595, 2127, 2636], "res": [], "img_idx": 0}
{"type": "table", "bbox": [1945, 361, 3343, 915], "res": {"cell_bbox": [[21.718862533569336, 13.258028030395508, 174.7668914794922, 13.647283554077148, 173.4757080078125, 60.153099060058594, 20.887605667114258, 59.64413833618164], [366.1514587402344, 11.42840576171875, 638.2271118164062, 11.590156555175781, 637.8952026367188, 64.84843444824219, 365.3397216796875, 65.1164779663086], [714.427978515625, 12.612360000610352, 1065.1962890625, 12.729161262512207, 1063.7314453125, 66.27478790283203, 713.4392700195312, 66.39193725585938], [1151.0953369140625, 11.123254776000977, 1343.7110595703125, 11.365469932556152, 1342.9759521484375, 71.91647338867188, 1150.9407958984375, 71.7182388305664], [12.874542236328125, 85.48368072509766, 220.92994689941406, 84.98846435546875, 219.86038208007812, 131.24313354492188, 12.516176223754883, 131.29539489746094], [347.1451416015625, 85.46978759765625, 646.7902221679688, 85.53042602539062, 646.0297241210938, 128.82183837890625, 345.04193115234375, 128.64764404296875], [744.375732421875, 85.99622344970703, 1021.3848876953125, 86.28846740722656, 1020.136962890625, 131.3214569091797, 742.1896362304688, 130.92080688476562], [1113.9969482421875, 84.71207427978516, 1348.4918212890625, 85.00597381591797, 1347.672119140625, 134.81214904785156, 1112.2056884765625, 134.63668823242188], [17.659879684448242, 141.31272888183594, 194.5921630859375, 141.43685913085938, 194.2716064453125, 185.34654235839844, 17.542705535888672, 185.03724670410156], [432.13214111328125, 140.8814697265625, 641.7965698242188, 140.9840545654297, 638.4487915039062, 185.32752990722656, 427.62225341796875, 185.20362854003906], [750.0819702148438, 140.75318908691406, 1023.2221069335938, 141.12550354003906, 1022.2069091796875, 187.6514892578125, 747.7850341796875, 187.2357177734375], [1108.11572265625, 140.7852783203125, 1347.130126953125, 141.58395385742188, 1346.330078125, 191.789306640625, 1106.041748046875, 191.02110290527344], [11.547578811645508, 197.04405212402344, 235.63177490234375, 198.14019775390625, 234.68040466308594, 248.71804809570312, 11.390583038330078, 247.4416046142578], [457.2102355957031, 198.858642578125, 635.3173217773438, 199.42098999023438, 631.4458618164062, 250.50645446777344, 451.66168212890625, 249.6580047607422], [765.123291015625, 198.78150939941406, 1019.2001342773438, 199.45458984375, 1017.4833984375, 251.67776489257812, 761.6964721679688, 250.9176483154297], [1118.51953125, 198.39993286132812, 1348.82470703125, 199.78515625, 1348.052001953125, 260.4172058105469, 1116.3905029296875, 259.1204528808594], [12.222949981689453, 275.442626953125, 219.95375061035156, 276.653564453125, 219.2496337890625, 334.5794372558594, 12.108200073242188, 333.2164306640625], [429.7825012207031, 275.4506530761719, 648.9439697265625, 276.5025329589844, 645.619140625, 337.9949645996094, 425.1380615234375, 336.8748779296875], [777.135498046875, 275.4131774902344, 1000.6702880859375, 276.7933044433594, 1000.3419189453125, 337.4403381347656, 775.6884155273438, 336.3292236328125], [1115.44775390625, 270.7770690917969, 1349.4019775390625, 272.385498046875, 1348.8214111328125, 343.7541809082031, 1114.080810546875, 342.73046875], [12.119233131408691, 379.0089111328125, 210.86831665039062, 379.6789245605469, 211.25326538085938, 432.0911865234375, 12.079060554504395, 431.27557373046875], [403.1966552734375, 376.5273132324219, 684.8984985351562, 377.0136413574219, 684.2984619140625, 441.0770263671875, 401.1081848144531, 440.4817810058594], [734.7579345703125, 372.3357849121094, 1054.611083984375, 373.13006591796875, 1054.61669921875, 446.97784423828125, 734.4263305664062, 446.3796081542969], [1136.1920166015625, 370.3330078125, 1330.1702880859375, 370.7735900878906, 1329.7821044921875, 435.013671875, 1135.7880859375, 434.9462890625], [12.789180755615234, 478.4594421386719, 234.01889038085938, 478.7506103515625, 235.75547790527344, 525.8236083984375, 12.809609413146973, 525.475830078125], [394.3120422363281, 475.2839660644531, 673.66455078125, 475.6749267578125, 674.6346435546875, 525.6235961914062, 393.8722839355469, 525.38623046875], [793.6969604492188, 474.5579833984375, 997.9990844726562, 474.66802978515625, 997.1876220703125, 527.1795654296875, 793.7205810546875, 527.1024169921875], [1132.2728271484375, 474.4861145019531, 1323.7657470703125, 474.6731872558594, 1323.519775390625, 529.9020385742188, 1133.085693359375, 529.8812255859375]], "html": "<html><body><table><thead><tr><td>Model</td><td>#Parameters</td><td>Training Dataset</td><td>Type</td></tr></thead><tbody><tr><td>GPT-Neo</td><td>125M/1.3B/2.7B</td><td>The Pile</td><td>Generative</td></tr><tr><td>GPT-J</td><td>6.7B</td><td>The Pile</td><td>Generative</td></tr><tr><td>GPT-NeoX</td><td>20B</td><td>The Pile</td><td>Generative</td></tr><tr><td>Codex</td><td>12B</td><td>N.R.</td><td>Generative & Infilling</td></tr><tr><td>CodeT5</td><td>220M</td><td>CodeSearchNet & BigQuery</td><td>Infilling</td></tr><tr><td>INCODER</td><td>1.3B/6.7B</td><td>N.R.</td><td>Infilling</td></tr></tbody></table></body></html>"}, "img_idx": 0}
{"type": "table_caption", "bbox": [2296, 289, 2958, 333], "res": [], "img_idx": 0}
