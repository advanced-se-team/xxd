{"type": "text", "bbox": [288, 316, 1805, 2588], "res": [{"text": "this task since the generation process conditions only on the", "confidence": 0.9776887893676758, "text_region": [[297.0, 321.0], [1793.0, 318.0], [1793.0, 366.0], [298.0, 368.0]]}, {"text": "context to the left (prefix). Therefore, for correct code infilling,", "confidence": 0.9853551387786865, "text_region": [[293.0, 377.0], [1798.0, 382.0], [1798.0, 446.0], [293.0, 441.0]]}, {"text": "we only use infilling models which perform generation by", "confidence": 0.9812569618225098, "text_region": [[297.0, 458.0], [1796.0, 458.0], [1796.0, 512.0], [297.0, 512.0]]}, {"text": "conditioning on both left (prefix) and right (suffix) code.", "confidence": 0.9877798557281494, "text_region": [[295.0, 527.0], [1689.0, 529.0], [1689.0, 583.0], [295.0, 581.0]]}, {"text": "Figure 2 shows an example input for the infilling task.", "confidence": 0.9636788368225098, "text_region": [[354.0, 598.0], [1793.0, 600.0], [1793.0, 654.0], [354.0, 652.0]]}, {"text": "We start with the target buggy function we want to fix and", "confidence": 0.9917030930519104, "text_region": [[297.0, 673.0], [1798.0, 673.0], [1798.0, 730.0], [297.0, 730.0]]}, {"text": "remove the buggy code hunk. This gives us the prefix and", "confidence": 0.9830016493797302, "text_region": [[300.0, 749.0], [1796.0, 749.0], [1796.0, 796.0], [300.0, 796.0]]}, {"text": "suffix code which are still correct. We then place an infilling", "confidence": 0.9895217418670654, "text_region": [[300.0, 808.0], [1798.0, 818.0], [1798.0, 875.0], [300.0, 865.0]]}, {"text": "token between the prefix and suffix. This infilling token (e.g.,", "confidence": 0.9886960387229919, "text_region": [[293.0, 877.0], [1796.0, 886.0], [1795.0, 950.0], [293.0, 941.0]]}, {"text": "<INFILL>) indicates to the model that this is the location", "confidence": 0.9725169539451599, "text_region": [[305.0, 960.0], [1793.0, 960.0], [1793.0, 1007.0], [305.0, 1007.0]]}, {"text": "where we want the new code to be generated at. The model", "confidence": 0.980770468711853, "text_region": [[300.0, 1033.0], [1795.0, 1031.0], [1796.0, 1080.0], [300.0, 1083.0]]}, {"text": "then generates only the code to fill in the missing chunk and", "confidence": 0.9925388097763062, "text_region": [[297.0, 1104.0], [1803.0, 1104.0], [1803.0, 1159.0], [297.0, 1159.0]]}, {"text": "we obtain a patch by combining the model output with the", "confidence": 0.9810315370559692, "text_region": [[295.0, 1173.0], [1796.0, 1173.0], [1796.0, 1227.0], [295.0, 1227.0]]}, {"text": "prefix and suffix code snippets.", "confidence": 0.9839523434638977, "text_region": [[295.0, 1248.0], [1058.0, 1248.0], [1058.0, 1303.0], [295.0, 1303.0]]}, {"text": "3) Single line generation: In single line generation, the", "confidence": 0.9813865423202515, "text_region": [[352.0, 1319.0], [1796.0, 1319.0], [1796.0, 1376.0], [352.0, 1376.0]]}, {"text": "buggy location is provided and the bug requires only a single", "confidence": 0.9788374304771423, "text_region": [[297.0, 1393.0], [1798.0, 1393.0], [1798.0, 1447.0], [297.0, 1447.0]]}, {"text": "line change. Figure 3a shows a similar setup to correct code", "confidence": 0.9838985204696655, "text_region": [[300.0, 1466.0], [1796.0, 1466.0], [1796.0, 1514.0], [300.0, 1514.0]]}, {"text": "infilling where we provide both the prefix and suffix code and", "confidence": 0.9781852960586548, "text_region": [[297.0, 1535.0], [1800.0, 1535.0], [1800.0, 1589.0], [297.0, 1589.0]]}, {"text": "use infilling models to generate a replacement line. Different", "confidence": 0.9827266335487366, "text_region": [[295.0, 1606.0], [1800.0, 1603.0], [1800.0, 1660.0], [295.0, 1663.0]]}, {"text": "from correct code infilling, we can also use generative models", "confidence": 0.9796146154403687, "text_region": [[295.0, 1677.0], [1796.0, 1677.0], [1796.0, 1731.0], [295.0, 1731.0]]}, {"text": "by providing only the prefix. Figure 3b demonstrates the setup", "confidence": 0.9820436835289001, "text_region": [[295.0, 1750.0], [1796.0, 1753.0], [1795.0, 1807.0], [295.0, 1805.0]]}, {"text": "to use generative models for this task. Since we know the bug", "confidence": 0.9934753775596619, "text_region": [[293.0, 1821.0], [1798.0, 1821.0], [1798.0, 1878.0], [293.0, 1878.0]]}, {"text": "requires only a single line change, we can stop the generation", "confidence": 0.9903482794761658, "text_region": [[297.0, 1895.0], [1798.0, 1895.0], [1798.0, 1949.0], [297.0, 1949.0]]}, {"text": "after the model has provided us with one line. We cannot", "confidence": 0.9894438982009888, "text_region": [[297.0, 1966.0], [1800.0, 1966.0], [1800.0, 2020.0], [297.0, 2020.0]]}, {"text": "apply the same strategy using generative models for correct", "confidence": 0.9812105298042297, "text_region": [[300.0, 2037.0], [1798.0, 2037.0], [1798.0, 2093.0], [300.0, 2093.0]]}, {"text": "code infilling since those bugs may need multiple lines to fix", "confidence": 0.9884386658668518, "text_region": [[295.0, 2105.0], [1796.0, 2108.0], [1795.0, 2164.0], [295.0, 2162.0]]}, {"text": "and we do not know when we can stop the generation [3i].", "confidence": 0.981199324131012, "text_region": [[295.0, 2176.0], [1796.0, 2181.0], [1795.0, 2235.0], [295.0, 2231.0]]}, {"text": "Additionally, when using generative models for single line", "confidence": 0.9909392595291138, "text_region": [[297.0, 2252.0], [1798.0, 2252.0], [1798.0, 2306.0], [297.0, 2306.0]]}, {"text": "generation, we cannot provide the models with the suffix code", "confidence": 0.9855814576148987, "text_region": [[295.0, 2328.0], [1795.0, 2323.0], [1796.0, 2373.0], [295.0, 2377.0]]}, {"text": "due to the causal nature of the generative models. We contrast", "confidence": 0.9854278564453125, "text_region": [[297.0, 2399.0], [1798.0, 2399.0], [1798.0, 2446.0], [297.0, 2446.0]]}, {"text": "this with infilling models on the same task to demonstrate the", "confidence": 0.9884417057037354, "text_region": [[297.0, 2470.0], [1791.0, 2470.0], [1791.0, 2517.0], [297.0, 2517.0]]}, {"text": "effect of including the suffix context for APR.", "confidence": 0.981940746307373, "text_region": [[302.0, 2538.0], [1426.0, 2538.0], [1426.0, 2586.0], [302.0, 2586.0]]}], "img_idx": 0}
{"type": "text", "bbox": [1869, 949, 3380, 1857], "res": [{"text": "Mean entropy averages entropies of a", "confidence": 0.9656476974487305, "text_region": [[1934.0, 950.0], [2891.0, 957.0], [2890.0, 1011.0], [1933.0, 1004.0]]}, {"text": "tokensgenerated", "confidence": 0.9970722794532776, "text_region": [[2947.0, 955.0], [3372.0, 955.0], [3372.0, 1004.0], [2947.0, 1004.0]]}, {"text": "?1", "confidence": 0.09828872978687286, "text_region": [[2881.0, 974.0], [2925.0, 974.0], [2925.0, 990.0], [2881.0, 990.0]]}, {"text": "whereas sum entropy computes the total entropy of the se-", "confidence": 0.9990549087524414, "text_region": [[1880.0, 1018.0], [3374.0, 1018.0], [3374.0, 1077.0], [1880.0, 1077.0]]}, {"text": "quence. For patch ranking, we prioritize patches with lower", "confidence": 0.9890759587287903, "text_region": [[1875.0, 1092.0], [3374.0, 1091.0], [3374.0, 1144.0], [1875.0, 1146.0]]}, {"text": "entropy first. In this way, patches that are more natural [47]", "confidence": 0.9793515801429749, "text_region": [[1886.0, 1165.0], [3372.0, 1162.0], [3372.0, 1214.0], [1886.0, 1217.0]]}, {"text": "can be ranked higher. Previous work on leveraging LLMs", "confidence": 0.9656650424003601, "text_region": [[1874.0, 1230.0], [3372.0, 1234.0], [3372.0, 1291.0], [1874.0, 1286.0]]}, {"text": "for APR either used mean entropy [26] or sum entropy [31]", "confidence": 0.9912189841270447, "text_region": [[1874.0, 1304.0], [3374.0, 1308.0], [3374.0, 1362.0], [1874.0, 1357.0]]}, {"text": "without thorough evaluation, and mainly focused on patch", "confidence": 0.9806255102157593, "text_region": [[1880.0, 1378.0], [3372.0, 1381.0], [3372.0, 1433.0], [1880.0, 1430.0]]}, {"text": "ranking. In contrast, in this work, we empirically compare both", "confidence": 0.9849792122840881, "text_region": [[1880.0, 1450.0], [3371.0, 1450.0], [3371.0, 1504.0], [1880.0, 1504.0]]}, {"text": "entropy computations, and have further applied them for patch", "confidence": 0.9983662366867065, "text_region": [[1883.0, 1520.0], [3374.0, 1516.0], [3374.0, 1575.0], [1883.0, 1578.0]]}, {"text": "correctness checking [58]. Finally, for each patch generated.", "confidence": 0.9812869429588318, "text_region": [[1880.0, 1595.0], [3369.0, 1595.0], [3369.0, 1647.0], [1880.0, 1647.0]]}, {"text": "we filter out any patches with syntactic or semantic errors", "confidence": 0.9755942225456238, "text_region": [[1875.0, 1665.0], [3374.0, 1663.0], [3374.0, 1717.0], [1875.0, 1718.0]]}, {"text": "and validate the rest against the test suites to identify patches", "confidence": 0.991562008857727, "text_region": [[1880.0, 1736.0], [3372.0, 1739.0], [3372.0, 1788.0], [1880.0, 1784.0]]}, {"text": "which pass all the tests.", "confidence": 0.9975076913833618, "text_region": [[1882.0, 1802.0], [2456.0, 1807.0], [2456.0, 1854.0], [1881.0, 1849.0]]}], "img_idx": 0}
{"type": "text", "bbox": [289, 2749, 1807, 4312], "res": [{"text": "For all 3 repair tasks, t", "confidence": 0.9756023287773132, "text_region": [[353.0, 2750.0], [991.0, 2759.0], [990.0, 2814.0], [353.0, 2806.0]]}, {"text": "the patch generation process", "confidence": 0.9941026568412781, "text_region": [[978.0, 2759.0], [1733.0, 2759.0], [1733.0, 2808.0], [978.0, 2808.0]]}, {"text": "S1S", "confidence": 0.9664822220802307, "text_region": [[1712.0, 2762.0], [1797.0, 2762.0], [1797.0, 2799.0], [1712.0, 2799.0]]}, {"text": "similar - we provide the LLMs with the constructed input", "confidence": 0.9804683923721313, "text_region": [[300.0, 2821.0], [1800.0, 2821.0], [1800.0, 2876.0], [300.0, 2876.0]]}, {"text": "and use sampling to generate multiple patches per bug. We use", "confidence": 0.9894300103187561, "text_region": [[299.0, 2891.0], [1800.0, 2892.0], [1800.0, 2953.0], [299.0, 2951.0]]}, {"text": "nucleus sampling [57] with a sampling temperature. A lower", "confidence": 0.9829569458961487, "text_region": [[297.0, 2967.0], [1799.0, 2967.0], [1799.0, 3023.0], [297.0, 3023.0]]}, {"text": "temperature means the model is likely to pick tokens with", "confidence": 0.9906946420669556, "text_region": [[300.0, 3039.0], [1796.0, 3037.0], [1796.0, 3088.0], [300.0, 3089.0]]}, {"text": "higher likelihood, resulting in samples that are more similar", "confidence": 0.9835565090179443, "text_region": [[299.0, 3104.0], [1799.0, 3106.0], [1799.0, 3166.0], [299.0, 3164.0]]}, {"text": "(temperature of O gives deterministic result by picking the most", "confidence": 0.9866376519203186, "text_region": [[302.0, 3182.0], [1799.0, 3184.0], [1799.0, 3234.0], [302.0, 3233.0]]}, {"text": "likely token at each generation step). A higher temperature", "confidence": 0.9770077466964722, "text_region": [[301.0, 3252.0], [1799.0, 3255.0], [1799.0, 3311.0], [300.0, 3307.0]]}, {"text": "gives more probability for the model to pick a token with", "confidence": 0.9785953760147095, "text_region": [[299.0, 3327.0], [1797.0, 3322.0], [1797.0, 3376.0], [299.0, 3381.0]]}, {"text": "a lower likelihood, leading to more unique and interesting", "confidence": 0.972773015499115, "text_region": [[289.0, 3392.0], [1799.0, 3397.0], [1799.0, 3456.0], [289.0, 3451.0]]}, {"text": "samples. How to pick an optimal temperature value is not", "confidence": 0.9852412343025208, "text_region": [[299.0, 3469.0], [1802.0, 3467.0], [1802.0, 3522.0], [299.0, 3524.0]]}, {"text": "obvious for a problem such as APR. For certain bugs, one may", "confidence": 0.9778628945350647, "text_region": [[302.0, 3537.0], [1794.0, 3540.0], [1794.0, 3596.0], [302.0, 3592.0]]}, {"text": "prefer a lower temperature value in order to quickly arrive at a", "confidence": 0.9911915063858032, "text_region": [[297.0, 3612.0], [1799.0, 3612.0], [1799.0, 3666.0], [297.0, 3666.0]]}, {"text": "reasonable patch. For harder bugs, a higher temperature value", "confidence": 0.9991880059242249, "text_region": [[296.0, 3680.0], [1794.0, 3685.0], [1794.0, 3741.0], [295.0, 3736.0]]}, {"text": "can be useful to generate more unique patches in an attempt to", "confidence": 0.9929100275039673, "text_region": [[301.0, 3755.0], [1792.0, 3758.0], [1792.0, 3809.0], [300.0, 3806.0]]}, {"text": "provide a fix. For our experiments, we use the default setting", "confidence": 0.9833971261978149, "text_region": [[297.0, 3828.0], [1794.0, 3828.0], [1794.0, 3882.0], [297.0, 3882.0]]}, {"text": "used in previous work [28], [33].", "confidence": 0.9919543266296387, "text_region": [[299.0, 3900.0], [1109.0, 3900.0], [1109.0, 3949.0], [299.0, 3949.0]]}, {"text": "In addition to generating patches, we also record the entropy", "confidence": 0.9922493100166321, "text_region": [[351.0, 3970.0], [1792.0, 3970.0], [1792.0, 4030.0], [351.0, 4030.0]]}, {"text": "value of each patch. Entropy captures how natural [47] the", "confidence": 0.9788296818733215, "text_region": [[304.0, 4043.0], [1794.0, 4043.0], [1794.0, 4099.0], [304.0, 4099.0]]}, {"text": "generated sample is according", "confidence": 0.9757678508758545, "text_region": [[299.0, 4113.0], [1094.0, 4112.0], [1094.0, 4167.0], [299.0, 4169.0]]}, {"text": "g to the model and can be", "confidence": 0.9412357211112976, "text_region": [[1066.0, 4117.0], [1797.0, 4115.0], [1797.0, 4161.0], [1066.0, 4162.0]]}, {"text": "calculated as the negative log probability of each generated", "confidence": 0.9885658025741577, "text_region": [[297.0, 4182.0], [1799.0, 4187.0], [1799.0, 4247.0], [297.0, 4242.0]]}, {"text": "token. Let ti,t2....", "confidence": 0.9430481195449829, "text_region": [[299.0, 4258.0], [772.0, 4262.0], [771.0, 4301.0], [299.0, 4297.0]]}, {"text": ",.be the list of tokens generated and", "confidence": 0.9450559616088867, "text_region": [[845.0, 4260.0], [1797.0, 4260.0], [1797.0, 4304.0], [845.0, 4304.0]]}], "img_idx": 0}
{"type": "text", "bbox": [1903, 2151, 3380, 3747], "res": [{"text": "RQl: How do different types of LLMs perform for", "confidence": 0.9590649008750916, "text_region": [[1929.0, 2186.0], [3372.0, 2186.0], [3372.0, 2241.0], [1929.0, 2241.0]]}, {"text": "different APR settings? We study the effectiveness of", "confidence": 0.9633976817131042, "text_region": [[1929.0, 2256.0], [3377.0, 2259.0], [3377.0, 2314.0], [1929.0, 2311.0]]}, {"text": "different LLMs on different repair datasets, across different", "confidence": 0.9841384887695312, "text_region": [[1929.0, 2332.0], [3375.0, 2332.0], [3375.0, 2382.0], [1929.0, 2382.0]]}, {"text": "languages and on different APR tasks. Furthermore, we", "confidence": 0.9864590764045715, "text_region": [[1924.0, 2402.0], [3373.0, 2402.0], [3373.0, 2457.0], [1924.0, 2457.0]]}, {"text": "evaluate the scaling behavior of LLMs when increasing", "confidence": 0.9786402583122253, "text_region": [[1926.0, 2470.0], [3375.0, 2474.0], [3375.0, 2530.0], [1926.0, 2527.0]]}, {"text": "model size with respect to APR ability, computation time", "confidence": 0.9803598523139954, "text_region": [[1929.0, 2547.0], [3370.0, 2547.0], [3370.0, 2602.0], [1929.0, 2602.0]]}, {"text": "and compilation rates to holistically evaluate each LLM.", "confidence": 0.9831968545913696, "text_region": [[1929.0, 2616.0], [3312.0, 2616.0], [3312.0, 2671.0], [1929.0, 2671.0]]}, {"text": "RQ2: How does directly applying LLMs for APR com-", "confidence": 0.9900996685028076, "text_region": [[1928.0, 2686.0], [3372.0, 2685.0], [3372.0, 2746.0], [1928.0, 2748.0]]}, {"text": "pare against state-of-the-art APR tools? We compare the", "confidence": 0.9908442497253418, "text_region": [[1926.0, 2763.0], [3367.0, 2758.0], [3367.0, 2814.0], [1926.0, 2819.0]]}, {"text": "results using", "confidence": 0.97022944688797, "text_region": [[1928.0, 2839.0], [2249.0, 2839.0], [2249.0, 2884.0], [1928.0, 2884.0]]}, {"text": "g LLMs against state-of-the-art baselines. We", "confidence": 0.9782422184944153, "text_region": [[2223.0, 2836.0], [3375.0, 2831.0], [3375.0, 2881.0], [2223.0, 2886.0]]}, {"text": "study the unique bugs fixed by LLMs and highlight the", "confidence": 0.9796363115310669, "text_region": [[1929.0, 2904.0], [3365.0, 2904.0], [3365.0, 2959.0], [1929.0, 2959.0]]}, {"text": "advantages of directly applying LLMs for APR.", "confidence": 0.9901952147483826, "text_region": [[1929.0, 2974.0], [3108.0, 2974.0], [3108.0, 3035.0], [1929.0, 3035.0]]}, {"text": "RQ3: Can LLMs be directly", "confidence": 0.9839630722999573, "text_region": [[1929.0, 3047.0], [2717.0, 3049.0], [2717.0, 3100.0], [1929.0, 3099.0]]}, {"text": "y used for patch ranking", "confidence": 0.9781519770622253, "text_region": [[2698.0, 3045.0], [3372.0, 3052.0], [3371.0, 3104.0], [2697.0, 3097.0]]}, {"text": "and correctness checking? We use the built-in naturalness", "confidence": 0.9933927655220032, "text_region": [[1929.0, 3122.0], [3375.0, 3122.0], [3375.0, 3172.0], [1929.0, 3172.0]]}, {"text": "metric of LLMs (entropy) to evaluate if LLMs considers", "confidence": 0.9766122102737427, "text_region": [[1928.0, 3192.0], [3375.0, 3192.0], [3375.0, 3247.0], [1928.0, 3247.0]]}, {"text": "patched functions to be more natural than buggy functions", "confidence": 0.991593062877655, "text_region": [[1928.0, 3263.0], [3373.0, 3263.0], [3373.0, 3318.0], [1928.0, 3318.0]]}, {"text": "and if entropy can directly rank the patches for patch ranking", "confidence": 0.9987682104110718, "text_region": [[1928.0, 3335.0], [3369.0, 3340.0], [3368.0, 3391.0], [1928.0, 3386.0]]}, {"text": "and correctness checking.", "confidence": 0.9972738027572632, "text_region": [[1928.0, 3403.0], [2553.0, 3411.0], [2552.0, 3463.0], [1928.0, 3454.0]]}, {"text": "RQ4: Can", "confidence": 0.9362421035766602, "text_region": [[1933.0, 3481.0], [2223.0, 3481.0], [2223.0, 3528.0], [1933.0, 3528.0]]}, {"text": " we furtheri", "confidence": 0.9172272682189941, "text_region": [[2198.0, 3479.0], [2600.0, 3483.0], [2600.0, 3528.0], [2198.0, 3524.0]]}, {"text": "improve the performance of", "confidence": 0.9674742817878723, "text_region": [[2582.0, 3481.0], [3378.0, 3481.0], [3378.0, 3531.0], [2582.0, 3531.0]]}, {"text": "LLMs? We explore two directions for further improving", "confidence": 0.9732294082641602, "text_region": [[1926.0, 3546.0], [3373.0, 3553.0], [3373.0, 3609.0], [1926.0, 3602.0]]}, {"text": "LLMs\u2019 performance for APR: 1) increasing the number of", "confidence": 0.9710810780525208, "text_region": [[1931.0, 3627.0], [3373.0, 3627.0], [3373.0, 3672.0], [1931.0, 3672.0]]}, {"text": "samples, and 2) combining LLMs with templates.", "confidence": 0.977727472782135, "text_region": [[1928.0, 3692.0], [3148.0, 3694.0], [3148.0, 3744.0], [1928.0, 3742.0]]}], "img_idx": 0}
{"type": "text", "bbox": [1868, 3900, 3381, 4309], "res": [{"text": "We implement the &", "confidence": 0.9109718799591064, "text_region": [[1939.0, 3906.0], [2459.0, 3915.0], [2459.0, 3956.0], [1939.0, 3948.0]]}, {"text": "generation pipeline", "confidence": 0.9830495119094849, "text_region": [[2442.0, 3911.0], [2930.0, 3913.0], [2930.0, 3956.0], [2442.0, 3954.0]]}, {"text": "1Pythonusing", "confidence": 0.9817766547203064, "text_region": [[2994.0, 3905.0], [3375.0, 3910.0], [3374.0, 3959.0], [2993.0, 3954.0]]}, {"text": "In", "confidence": 0.4938197731971741, "text_region": [[2911.0, 3923.0], [3006.0, 3917.0], [3008.0, 3946.0], [2912.0, 3952.0]]}, {"text": "PyTorch [59] versions of each LLM. We use the Hugging", "confidence": 0.9858331680297852, "text_region": [[1874.0, 3967.0], [3375.0, 3970.0], [3375.0, 4029.0], [1874.0, 4026.0]]}, {"text": "Face [48] to load the model weights and generate outputs. For", "confidence": 0.9909627437591553, "text_region": [[1873.0, 4037.0], [3376.0, 4041.0], [3376.0, 4105.0], [1873.0, 4101.0]]}, {"text": "Codex, we use API access provided by OpenAI to query the", "confidence": 0.9867900013923645, "text_region": [[1879.0, 4112.0], [3368.0, 4116.0], [3368.0, 4170.0], [1879.0, 4167.0]]}, {"text": "model [60]. To use Codex for correct code infilling, we append", "confidence": 0.9787226319313049, "text_region": [[1874.0, 4183.0], [3378.0, 4188.0], [3378.0, 4242.0], [1874.0, 4237.0]]}, {"text": "the API request with an additional suffix parameter [55] with", "confidence": 0.9762023687362671, "text_region": [[1874.0, 4255.0], [3370.0, 4255.0], [3370.0, 4303.0], [1874.0, 4303.0]]}], "img_idx": 0}
{"type": "text", "bbox": [1873, 315, 3371, 503], "res": [{"text": "tokentiglven", "confidence": 0.8442423343658447, "text_region": [[2923.0, 330.0], [3284.0, 330.0], [3284.0, 365.0], [2923.0, 365.0]]}, {"text": "O?", "confidence": 0.2504884600639343, "text_region": [[2586.0, 343.0], [2642.0, 343.0], [2642.0, 359.0], [2586.0, 359.0]]}, {"text": "\u4e00", "confidence": 0.1972941756248474, "text_region": [[2725.0, 347.0], [2733.0, 347.0], [2733.0, 356.0], [2725.0, 356.0]]}, {"text": "previous context and generated tokens. Entropy is defined as", "confidence": 0.9950971007347107, "text_region": [[1879.0, 388.0], [3360.0, 385.0], [3360.0, 437.0], [1879.0, 440.0]]}], "img_idx": 0}
{"type": "text", "bbox": [2280, 1923, 2978, 1968], "res": [], "img_idx": 0}
{"type": "text", "bbox": [3306, 595, 3376, 835], "res": [{"text": "2", "confidence": 0.9775078892707825, "text_region": [[3314.0, 792.0], [3362.0, 792.0], [3362.0, 829.0], [3314.0, 829.0]]}], "img_idx": 0}
{"type": "text", "bbox": [3307, 781, 3373, 837], "res": [{"text": "2", "confidence": 0.9910502433776855, "text_region": [[3313.0, 791.0], [3363.0, 791.0], [3363.0, 831.0], [3313.0, 831.0]]}], "img_idx": 0}
{"type": "text", "bbox": [3306, 596, 3375, 699], "res": [], "img_idx": 0}
{"type": "title", "bbox": [1869, 3809, 2327, 3857], "res": [{"text": "B. Implementation", "confidence": 0.993758499622345, "text_region": [[1869.0, 3809.0], [2326.0, 3809.0], [2326.0, 3855.0], [1869.0, 3855.0]]}], "img_idx": 0}
{"type": "title", "bbox": [1866, 2012, 2433, 2065], "res": [{"text": "A.", "confidence": 0.9446337819099426, "text_region": [[1876.0, 2023.0], [1918.0, 2023.0], [1918.0, 2052.0], [1876.0, 2052.0]]}, {"text": "Research", "confidence": 0.9993094205856323, "text_region": [[1950.0, 2016.0], [2175.0, 2016.0], [2175.0, 2056.0], [1950.0, 2056.0]]}, {"text": "Questions", "confidence": 0.9625268578529358, "text_region": [[2191.0, 2018.0], [2429.0, 2020.0], [2429.0, 2058.0], [2191.0, 2056.0]]}], "img_idx": 0}
{"type": "title", "bbox": [294, 2653, 1105, 2708], "res": [{"text": "C.", "confidence": 0.9353718757629395, "text_region": [[301.0, 2663.0], [351.0, 2663.0], [351.0, 2697.0], [301.0, 2697.0]]}, {"text": "Patch", "confidence": 0.9967104196548462, "text_region": [[379.0, 2662.0], [519.0, 2662.0], [519.0, 2699.0], [379.0, 2699.0]]}, {"text": "1 Ranking", "confidence": 0.9073723554611206, "text_region": [[500.0, 2660.0], [750.0, 2665.0], [750.0, 2701.0], [500.0, 2697.0]]}, {"text": " and", "confidence": 0.874215304851532, "text_region": [[719.0, 2666.0], [843.0, 2663.0], [844.0, 2698.0], [721.0, 2701.0]]}, {"text": "Validation", "confidence": 0.9913322329521179, "text_region": [[865.0, 2659.0], [1101.0, 2662.0], [1100.0, 2700.0], [865.0, 2698.0]]}], "img_idx": 0}
{"type": "equation", "bbox": [2239, 544, 3007, 906], "res": [{"text": "log(pti", "confidence": 0.9362125396728516, "text_region": [[2818.0, 550.0], [2994.0, 565.0], [2990.0, 616.0], [2814.0, 601.0]]}, {"text": "mean_entropy", "confidence": 0.998500406742096, "text_region": [[2239.0, 596.0], [2603.0, 604.0], [2602.0, 649.0], [2239.0, 641.0]]}, {"text": ">", "confidence": 0.45653223991394043, "text_region": [[2738.0, 608.0], [2774.0, 608.0], [2774.0, 648.0], [2738.0, 648.0]]}, {"text": "\u4e00", "confidence": 0.6217982769012451, "text_region": [[2631.0, 618.0], [2654.0, 618.0], [2654.0, 636.0], [2631.0, 636.0]]}, {"text": "n", "confidence": 0.9330360293388367, "text_region": [[2895.0, 647.0], [2936.0, 647.0], [2936.0, 683.0], [2895.0, 683.0]]}, {"text": "i=1", "confidence": 0.8369863629341125, "text_region": [[2719.0, 679.0], [2793.0, 675.0], [2794.0, 706.0], [2720.0, 710.0]]}, {"text": "n", "confidence": 0.9557343125343323, "text_region": [[2744.0, 732.0], [2774.0, 734.0], [2772.0, 760.0], [2741.0, 757.0]]}, {"text": "sum_entropy", "confidence": 0.998879075050354, "text_region": [[2263.0, 788.0], [2603.0, 796.0], [2602.0, 843.0], [2262.0, 835.0]]}, {"text": ">", "confidence": 0.6873257160186768, "text_region": [[2720.0, 781.0], [2792.0, 781.0], [2792.0, 853.0], [2720.0, 853.0]]}, {"text": "log(pti", "confidence": 0.9613569974899292, "text_region": [[2811.0, 785.0], [2989.0, 792.0], [2986.0, 848.0], [2808.0, 841.0]]}, {"text": "\u4e00", "confidence": 0.4251942038536072, "text_region": [[2625.0, 808.0], [2660.0, 808.0], [2660.0, 832.0], [2625.0, 832.0]]}, {"text": "=1", "confidence": 0.9749908447265625, "text_region": [[2718.0, 871.0], [2796.0, 869.0], [2797.0, 903.0], [2719.0, 905.0]]}], "img_idx": 0}
