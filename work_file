{
  "Title": "Automated Program Repair in the Era of Large Pre-trained Language Models",
  "Navigation Bra": "null",
  "AuthorInfo": "Chunqiu Steven Xia University of Illinois Urbana-Champaign chunqiu2@illinois.edu\nYuxiang Wei University of Illinois Urbana-Champaign ywei40@illinois.edu\nLingming Zhang University of Illinois Urbana-Champaign lingming@illinois.edu",
  "Abstract": "Abstract\u2014Automated Program Repair (APR) aims to help developers automatically patch software bugs. However, current state-of-the-art traditional and learning-based APR techniques face the problem of limited patch variety, failing to fix com- plicated bugs. This is mainly due to the reliance on bug-fixing datasets to craft fix templates (traditional) or directly predict potential patches (learning-based). Large Pre-Trained Language Models (LLMs), trained using billions of text/code tokens, can potentially help avoid this issue. Very recently, researchers have directly leveraged LLMs for APR without relying on any bug- fixing datasets. Meanwhile, such existing work either failed to include state-of-the-art LLMs or was not evaluated on realistic datasets. Thus, the true power of modern LLMs on the important APR problem is yet to be revealed.\nIn this work, we perform the first extensive study on directly applying LLMs for APR. We select 9 recent state-of-the-art LLMs, including both generative and infilling models, ranging from 125M to 20B in size. We designed 3 different repair settings to evaluate the different ways we can use LLMs to generate patches: 1) generate the entire patch function, 2) fill in a chunk of code given the prefix and suffix 3) output a single line fix. We apply the LLMs under these repair settings on 5 datasets across 3 different languages and compare different LLMs in the number of bugs fixed, generation speed and compilation rate. We also compare the LLMs against recent state-of-the-art APR tools. Our study demonstrates that directly applying state-of- the-art LLMs can already substantially outperform all existing APR techniques on all our datasets. Among the studied LLMs, the scaling effect exists for APR where larger models tend to achieve better performance. Also, we show for the first time that suffix code after the buggy line (adopted in infilling-style APR) is important in not only generating more fixes but more patches with higher compilation rate. Besides patch generation, the LLMs consider correct patches to be more natural than other ones, and can even be leveraged for effective patch ranking or patch correctness checking. Lastly, we show that LLM-based APR can be further substantially boosted via: 1) increasing the sample size, and 2) incorporating fix template information.",
  "Content": [
    {
      "Title": "INTRODUCTION",
      "Content": "\nAs software programs and systems become more and more ubiquitous in everyday life, so do software bugs. Due to the wide-ranging adoption of software systems in fields from healthcare [1] to transportation [2], these bugs can potentially cause dangerous safety issues [3] and financial losses [4]. As such, developers often need to spend a significant amount of time and effort to fix software bugs [5]. In order to help developers reduce this manual effort, Automated Program\nRepair (APR) tools have been built to automatically generate potential patches given the original buggy program [6].\nAmong traditional APR techniques [7]\u2013[18], template-based APR has been widely recognized as the state of the art [19], [20]. These techniques leverage fix templates, often designed by human experts, to fix specific types of bugs in the source code. As a result, these APR tools are constrained by the underlying fix templates in the types of bugs that can be fixed. To combat this, researchers have proposed learning- based APR tools [21]\u2013[24], which typically model program repair as a Neural Machine Translation (NMT) problem [25], where the goal is to translate a buggy program into a fixed program. The core component of these learning-based APR tools is an encoder and decoder pair, where the model aims to capture the buggy context via the encoder and then autore- gressively generate the patch using the decoder. As such, these learning-based APR tools require supervised training datasets containing pairs of buggy and patched code, usually obtained by mining historical bug fixes from open-source repositories. While learning-based APR tools have shown improvements in both the number and variety of bugs that can be fixed [21], [22], they are still restricted by their training data which may contain unrelated commits and only contain limited bug-fix types, which may not generalize to unseen bug types [26].\nRecent developments in building Large Pre-Trained Lan- guage Models (LLMs) offer an alternative solution that can be applied for program repair without relying on historical bug fixes. While LLMs are usually general-purpose tools for NLP tasks (e.g., GPT3 [27]), they have also been used for pro- gramming languages by finetuning on code (e.g., Codex [28] and ChatGPT [29]). Unlike the specifically designed learning- based APR models, LLMs are trained in an unsupervised fashion using up to billions of text/code tokens and can be used in a variety of code tasks. Recently, AlphaRepair [26] proposes to leverage CodeBERT [30], a large code model pre-trained on millions of code snippets, directly for APR. The key insight from AlphaRepair is instead of learning transformations to go from buggy code to fixed code, we can directly use the model to predict what the correct code should look like given its surrounding context (including both prefix and suffix), i.e., infilling-style APR. Using this idea, AlphaRepair demonstrated state-of-the-art repair results without finetuning on bug fixing dataset. While AlphaRepair has shown improvements over\nprevious learning-based APR, the model (125M parameters) it uses is far smaller than the current state-of-the-art LLMs (Codex: 12B parameters and GPT-3: 175B parameters). Beside AlphaRepair, researchers have also directly leveraged Codex for generative APR [31], [32], i.e., generating the fixes based on the context before bugs (i.e., prefix only). However, these studies mostly focus on Codex and are only evaluated on a small dataset with 40 bugs on simple programming tasks.\nCurrent state-of-the-art LLMs [28], [33] have also included evaluation for code related tasks such as code completion [28], docstring generation [34] and variable/type prediction [34]. However, these evaluations still mainly focus on NLP metrics such as BLEU score [35] which do not accurately measure the functional or semantic correctness of the generated code. Fur- thermore, the datasets consist of hand-curated code problems which do not accurately reflect the type of projects developers work on in the real world.\nOur Work. We present the first extensive evaluation of recent LLMs for fixing real-world projects. We designed 3 different APR experimental settings: 1) complete function generation 2) correct code infilling and 3) single line generation to showcase the different ways LLMs can be applied for APR. In our study, we include both popular types of LLM architectures (generative and infilling models) to show the advantages and flaws of using each type for APR. We include models with a wide range of different parameter sizes, spanning from 125 million to 20 billion. We evaluate not only the improvement in repair effectiveness but also the trade-off with respect to speed when increasing the model size. In total, we use 5 different repair datasets containing real open-source bugs and developer written tests across 3 programming languages to evaluate APR under realistic settings. Compared with existing applications of LLMs for APR [26], [31], [32], our study is the first to include state-of-the-art LLMs for both infilling-style and generative APR on various datasets and programming languages. To summarize, this paper makes the following contributions.\n\u22c6 Dimension. This paper bridges the gap between the re- cent advances in LLMs and a crucial software engineering problem \u2013 APR. This paper not only demonstrates the potential and future for directly leveraging LLMs for solving the important APR problem, but also provides a realistic evaluation scenario for the recent LLMs, which were mainly evaluated on simple/synthetic coding problems rather than real-world systems as studied in the APR area.\n\u22c6 Study. We conduct extensive evaluations using 9 different recent LLMs on 5 different repair datasets across 3 different programming languages (Java, Python, and C). We compare the LLMs against each other using the 3 repair settings we designed. Using the popular repair datasets, we further compare the LLMs with state-of-the-art APR tools.\n\u22c6 Practical Guidelines. Our study shows for the first time that directly applying state-of-the-art LLMs can already substantially outperform all existing APR tools on the widely studied Defects4J 1.2 dataset (and other ones), e.g., Codex can fix 32 more bugs than the existing best APR\ntechnique. Among the studied LLMs, the scaling effect exists for APR where larger models tend to deliver stronger APR results. Also, we show for the first time that suffix code after the buggy line (adopted in infilling-style APR) is important in not only generating more fixes but more patches with higher compilation rate. Besides patch generation, the LLMs consider correct patches to be more natural than other ones, and can even be used for effective patch ranking or correctness checking. Lastly, we show that LLM-based APR can be further substantially improved via: 1) increasing the sample size, and 2) incorporating fix template information.",
      "Below": [],
      "Grade": 1
    },
    {
      "Title": "BACKGROUND AND RELATED WORK",
      "Content": "",
      "Below": [
        {
          "Title": "Large Pre-Trained Language Model",
          "Content": "\nLarge Pre-Trained Language Models (LLMs) have become ubiquitous in the domain of NLP, achieving impressive per- formance in many tasks such as machine translation [25], text summarization [36] and classification [37]. LLMs follow the Transformer architecture [38] \u2013 an encoder to capture input representation and a decoder to generate output tokens. These LLMs are first pre-trained in an unsupervised manner, on large amounts of text data and then finetuned for downstream tasks. However, certain tasks may not have an abundance of finetuned data available. As such, researchers have evaluated the ability for LLMs to perform on downstream tasks without finetuning. This is achieved via prompt engineering [39] \u2013 providing the model with natural language descriptions and demonstrations of the task it is trying to solve before giving the model the target input. This works by leveraging the general- purpose setup of LLMs where the unsupervised pretraining dataset already encompasses many domains of problems/tasks. Using this idea and the exponential growth in LLM size [40], impressive performance in many tasks can be achieved even without any finetuning [27].\nLLMs can be classified into encoder-only, decoder-only and encoder-decoder models based on their architectures. Encoder- only models (such as BERT [41]) contain only the encoder component of a Transformer. They are typically designed to learn data representations and are trained using the Masked Language Modeling (MLM) objective \u2013 a small percentage (e.g., 15%) of tokens in the training data will be replaced by masked tokens, and then the models are trained to predict the original values of the masked tokens based on the bidirectional contexts. Decoder-only models (such as GPT-3 [27] and GPT- Neo [42]) are large generative models that use the decoder to predict the next token output given all previous tokens (i.e., left context or prefix only). To combine the usage of both encoder and decoder, encoder-decoder models (such as T5 [43] and BART [44]) have also been proposed for sequence-to-sequence tasks where the training objective aims to recover the correct output sequence given the original input (e.g., corrupted to uncorrupted). One such training objective is span prediction tasks, where random spans (multiple tokens) are replaced with artificial span tokens and the model is tasked with recovering the original tokens. For inferencing, one can use the encoder- decoder models to infill text by also adding the artificial\nspan token in place. Recently, researchers have also combined MLM with generative models to perform both bidirectional and autoregressive text generation or infilling [45]. In our APR scenario, all types of LLMs can potentially be leveraged for generative or infilling-style APR, and we select 9 state-of-the- art LLMs for our study (detailed in Section III-A).\nTABLE I: Studied LLMs\nCodex\t12B\tN.R.\tGenerative",
          "Below": [],
          "Grade": 2
        },
        {
          "Title": "Automated Program Repair",
          "Content": "\nCodeT5\t220M\tCodeSearchNet\nInfilling\nAutomated Program Repair (APR) tools are used to generate patched code given the original code and the corresponding buggy location. Each patch generated by the APR tool is validated against the test suite. Plausible patches are ones which pass the entire suite. Correct patches are plausible patches which correctly fix the underlying bug.\nTraditional APR tools can be classified as heuristic- based [7]\u2013[9], constraint-based [10]\u2013[12] and template- based [13]\u2013[16], [19]. Traditionally, template-based APR tools achieve the best performance, where each template is hand- crafted by human experts designed to provide a fix for a specific type of bug. However, these template-based APR tools can only fix the bug types that are part of the templates. As a result, researchers employed learning-based APR tools to generate more expressive patches. Learning-based APR tools such as Recoder [21], RewardRepair [23], and CURE [22] are based on NMT techniques [25] which require specific bug fixing data to train the NMT model to generate a fix line given the buggy line. Due to this reliance on the bug-fixing data, these learning-based tools are still limited in terms of the type of fixes it can apply. Recent work of AlphaRepair [26] addresses this by performing APR under a zero-shot setting by directly using the CodeBERT model for repair. AlphaRepair fills the original buggy line with masked tokens and uses CodeBERT to replace the masked tokens with correct code tokens to generate repair, i.e., infilling-style (also called cloze- style) APR. While AlphaRepair is able to achieve state-of- the-art results, CodeBERT is considerably smaller than the newest LLMs. Additionally, AlphaRepair is designed for the repair setting where the buggy line location is known (e.g., computed by fault localization techniques [46]).\nRecent work [31], [32] has also looked into directly apply- ing LLMs for APR. Prenner et al. [32] conducted a small-scale evaluation for the Codex model on a simple dataset containing both Java and Python versions of buggy algorithm imple- mentations. Codex is given the buggy function and by using prompt engineering, are then asked to generate a complete fixed function. The results show that Codex is competitive with state-of-the-art learning-based APR tools in Python but worse in Java. In contrast, we show that by using our repair settings, LLMs are able to outperform state-of-the-art APR tools on both Java and Python. Kolak et al. [31] also used Codex along with 2 smaller LLMs and evaluated their ability to generate the correct patch line when given the code prefix on the same dataset as the previous work [32]. The evaluation demonstrated the scaling effect of LLMs where the repair results can be improved by using larger models. Interestingly, the study leverages sum entropy for patch ranking while\nINCODER\t1.3B/6.7B\tN.R.\tInfilling\nAlphaRepair leverages mean entropy (i.e., both favors more natural [47] patches). Thus, we also perform a study of leveraging various recent LLMs for computing both entropies for patch ranking on real-world systems. In addition, to the best of our knowledge, we are the first to study LLMs or entropies for patch correctness checking (i.e., distinguishing correct patches from plausible ones).\nOverall, the 2 prior studies [31], [32] are done on a small dataset with synthetic bugs using only a small number of LLMs. Moreover, the input and repair setting being used in the studies are also limited, e.g., only considered generative APR. In this paper, we present an extensive study of applying various state-of-the-art LLMs for both infilling-style and generative APR on diverse repair datasets across programming languages.",
          "Below": [],
          "Grade": 2
        }
      ],
      "Grade": 1
    },
    {
      "Title": "APPROACH",
      "Content": "\nIn this section we describe the LLMs selected for evaluation and introduce 3 different APR generation settings we use to evaluate each LLM. These settings are designed to showcase the different practical ways we can directly use LLMs for APR and highlight advantages and differences of the studied LLM types. Also, we detail the patch ranking strategy of using entropy to prioritize patches that are more likely to be correct.",
      "Below": [
        {
          "Title": "Models",
          "Content": "\nWe begin by describing the different LLMs we use for evaluation. Our selection process starts with the list of popular models hosted on the Hugging Face [48] \u2013 an open-source platform to host and deploy large models. We sort the list of models based on popularity (#downloads this month) and select the LLMs which contain code as training data. Fur- thermore, we also pick models from different organizations and types (described below) to obtain a diverse set of models. Along with the open-source models, we also use the closed- source Codex model [28] (accessible only via API) since it has shown to achieve impressive performance on code related tasks. In total, we use 9 different LLMs for our experiment.\nOur chosen LLMs range from 125M to 20B in parameter size. Table I presents the LLM overview. Column Model is the model name, #Parameters presents the number of model parameters, Training Dataset indicates the dataset used for pre-training (N.R. is not released), and Type refers to the type of APR the model can perform (infilling or generative).\nGenerative Models:\nGPT-Neo [42], GPT-J [49], GPT-NeoX [50] All three\nmodels are open-source implementations of the GPT-3 trans- former architecture [27]. In our experiments, we use GPT- Neo models with 125M, 1.3B and 2.7B parameters. GPT-J\nand GPT-NeoX are even larger models with 6.7B and 20B parameters. These models were trained on The Pile [51], an 800GB dataset combining 22 diverse text-based datasets with 7.6% containing open-source Github code.\nCodex [28] A 12B parameter GPT-3 based model designed for code generation. Codex is initialized with GPT-3 weights trained on natural language corpus and then finetuned on a large corpus of 159GB code files.\nInfilling Models:\nCodeT5 [52] A 220M parameter model based on T5 [43] ar- chitecture designed for code related tasks. CodeT5 is trained using span prediction objective on 8.35 million functions across 8 different programming languages by combining CodeSearchNet [53] with C/C# dataset from BigQuery [54].\n\tINCODER [33] A model designed for code infilling by adopting a causal masking objective [45]. INCODER is trained on both open-source Github/GitLab code (159 GB) and StackOverFlow questions and answers (57 GB). We use both the 1.3B and 6.7B parameter version.\nCodex In addition to using Codex as a generative model, we use the recently added suffix feature [55] to perform code infilling. Since Codex is not open-sourced, we do not know how the model performs the infilling.",
          "Below": [],
          "Grade": 2
        },
        {
          "Title": "LLM-based Patch Generation",
          "Content": "\nIn our study, we designed three settings for APR:\nComplete function generation \u2013 the input is a buggy function and the goal is to output the patched function.\nCorrect code infilling \u2013 the buggy location is known and the goal is to generate the correct replacement code given the prefix and suffix of the buggy function.\nSingle line generation \u2013 the bug location is provided and the bug is fixed by a single line change. Single line generation uses a subset of bugs in correct code infilling. We separate this case since many fault-localization techniques provides a ranking in the granularity of individual code lines [46], [56]. More importantly, both infilling and generative LLMs can be applied for this setting, enabling direct comparison of the two. We now describe the different inputs for each setting.\nComplete function generation: For this setting, the initial input is the original buggy function. We aim to use a generative model to autoregressively generate the entire patched version of the buggy function. However, naively feeding the LLMs the buggy function will not work since each LLM is not pre- trained for APR (i.e., they do not know that the goal is to generate a patched function). Therefore, to facilitate the direct usage of LLMs for APR, we use specific prompts to enable the models to perform few-shot learning. This allows the LLMs to recognize the task and generate a patched function by completing the input provided. We note here that the task of\nBuggy\nFig. 1: APR input for complete function generation\nprefix\nBuggy Function\nsuffix\nFig. 2: APR input for correct code infilling\nproject/dataset the target bug is from) in order to demonstrate the task and the expected format of the output. To start off, we follow the prior study [32] and begin with a description of the task: # Provide a fix for the buggy function. This describes in natural language the task we want the LLM to perform. This is a Python example and we use the Python comment format of # as a prefix for this description (we use other comment prefixes depending on the language of the buggy code). We then provide an example bug and patch pair. In Figure 1, this example is a function which computes the Fibonacci number. We prefix the example buggy and fixed function with # Buggy Function and # Fixed Function to provide additional context for the model. For our second example, we follow the same prompting style and pick a buggy and patched function pair from the same project that the bug is from. This way we can provide the model with some examples of the coding style used in the project. Finally, we finish the prompt by adding the bug we want to fix.\nCorrect code infilling: Unlike complete function gen- eration, where the buggy location within the function is not known. For correct code infilling, the input is the prefix and suffix after removing the buggy code hunk. In order to fill in the correct code, both the prefix and suffix can provide useful information. As a result, generative models are not suitable for\ncomplete function generation makes no assumption of 1) the location of the bug and 2) the type of bug or fix required. Therefore, the LLM needs to figure out why the function is buggy and provide a patch to fix the bug.\nFigure 1 shows the input which is made up of two ex-\nBuggy Function\nprefix\nsuffix\nBuggy Function\nprefix\nb)\nample bug fixes (one crafted by us and one from the same\nFig. 3: APR input for single line generation\nthis task since the generation process conditions only on the context to the left (prefix). Therefore, for correct code infilling, we only use infilling models which perform generation by\npti be the model probability of generating token ti given the previous context and generated tokens. Entropy is defined as:\nconditioning on both left (prefix) and right (suffix) code. Figure 2 shows an example input for the infilling task.\nWe start with the target buggy function we want to fix and\nmean entropy = \u2212\n\u03a3\ni=1 n\nlog(pti )\nn\n(1)\nremove the buggy code hunk. This gives us the prefix and\nsuffix code which are still correct. We then place an infilling token between the prefix and suffix. This infilling token (e.g.,\n<INFILL>) indicates to the model that this is the location where we want the new code to be generated at. The model then generates only the code to fill in the missing chunk and we obtain a patch by combining the model output with the prefix and suffix code snippets.\nSingle line generation: In single line generation, the buggy location is provided and the bug requires only a single line change. Figure 3a shows a similar setup to correct code infilling where we provide both the prefix and suffix code and use infilling models to generate a replacement line. Different from correct code infilling, we can also use generative models by providing only the prefix. Figure 3b demonstrates the setup to use generative models for this task. Since we know the bug requires only a single line change, we can stop the generation after the model has provided us with one line. We cannot apply the same strategy using generative models for correct code infilling since those bugs may need multiple lines to fix and we do not know when we can stop the generation [31]. Additionally, when using generative models for single line generation, we cannot provide the models with the suffix code due to the causal nature of the generative models. We contrast this with infilling models on the same task to demonstrate the effect of including the suffix context for APR.",
          "Below": [],
          "Grade": 2
        },
        {
          "Title": "Patch Ranking and Validation",
          "Content": "\nFor all 3 repair tasks, the patch generation process is similar \u2013 we provide the LLMs with the constructed input and use sampling to generate multiple patches per bug. We use nucleus sampling [57] with a sampling temperature. A lower temperature means the model is likely to pick tokens with higher likelihood, resulting in samples that are more similar (temperature of 0 gives deterministic result by picking the most likely token at each generation step). A higher temperature gives more probability for the model to pick a token with a lower likelihood, leading to more unique and interesting samples. How to pick an optimal temperature value is not obvious for a problem such as APR. For certain bugs, one may prefer a lower temperature value in order to quickly arrive at a reasonable patch. For harder bugs, a higher temperature value can be useful to generate more unique patches in an attempt to provide a fix. For our experiments, we use the default setting used in previous work [28], [33].\nIn addition to generating patches, we also record the entropy value of each patch. Entropy captures how natural [47] the generated sample is according to the model and can be calculated as the negative log probability of each generated token. Let t1, t2, ..., tn be the list of tokens generated and\nsumentropy = \u2212\u03a3 log(pti )\t(2)\ni=1\nMean entropy averages entropies of all tokens generated whereas sum entropy computes the total entropy of the se- quence. For patch ranking, we prioritize patches with lower entropy first. In this way, patches that are more natural [47] can be ranked higher. Previous work on leveraging LLMs for APR either used mean entropy [26] or sum entropy [31] without thorough evaluation, and mainly focused on patch ranking. In contrast, in this work, we empirically compare both entropy computations, and have further applied them for patch correctness checking [58]. Finally, for each patch generated, we filter out any patches with syntactic or semantic errors and validate the rest against the test suites to identify patches which pass all the tests.",
          "Below": [],
          "Grade": 2
        }
      ],
      "Grade": 1
    },
    {
      "Title": "EXPERIMENTAL SETUP",
      "Content": "",
      "Below": [
        {
          "Title": "Research Questions",
          "Content": "\nWe study the following research questions:\nRQ1: How do different types of LLMs perform for different APR settings? We study the effectiveness of different LLMs on different repair datasets, across different languages and on different APR tasks. Furthermore, we evaluate the scaling behavior of LLMs when increasing model size with respect to APR ability, computation time and compilation rates to holistically evaluate each LLM.\nRQ2: How does directly applying LLMs for APR com- pare against state-of-the-art APR tools? We compare the results using LLMs against state-of-the-art baselines. We study the unique bugs fixed by LLMs and highlight the advantages of directly applying LLMs for APR.\nRQ3: Can LLMs be directly used for patch ranking and correctness checking? We use the built-in naturalness metric of LLMs (entropy) to evaluate if LLMs considers patched functions to be more natural than buggy functions and if entropy can directly rank the patches for patch ranking and correctness checking.\nRQ4: Can we further improve the performance of LLMs? We explore two directions for further improving LLMs\u2019 performance for APR: 1) increasing the number of samples, and 2) combining LLMs with templates.",
          "Below": [],
          "Grade": 2
        },
        {
          "Title": "Implementation",
          "Content": "\nWe implement the generation pipeline in Python using PyTorch [59] versions of each LLM. We use the Hugging Face [48] to load the model weights and generate outputs. For Codex, we use API access provided by OpenAI to query the model [60]. To use Codex for correct code infilling, we append the API request with an additional suffix parameter [55] with\nTABLE II: Evaluation dataset statistics\tTABLE III: Complete function APR (SF bugs)\nDataset\t#Bugs\t#SF\t#SH\t#SL\tSource\tLanguage\nDataset\tGPT-Neo GPT-Neo GPT-Neo\nGPT-J  GPT-\nCodex\nDefects4J 1.2\t391\t255\t154\t80\treal-world\tJava\n125M\t1.3B\t2.7B\nNeoX\nDefects4J 2.0\t438\t228\t159\t78\treal-world\tJava\tDefects4J 1.2\t6 / 8\t7 / 16\t10 / 24  14 / 31 18 / 36 63 / 102\nQuixBugs- Java\ncoding problems\nJava\nDefects4J 2.0\t2 / 17\t4 / 18\t6 / 20  11 / 33 15 / 36 49 / 93\nQuixBugs-Java\t1 / 3\t4 / 5\t3 / 5\t3 / 5\t8 / 9  32 / 35\nQuixBugs-Py\t1 / 3\t4 / 6\t4 / 6\t13 / 17 19 / 22 37 / 37\nManyBugs\t0 / 2\t1 / 4\t2 / 4\t3 / 6  4 / 12  7 / 15\nTABLE IV: Correct code infilling APR (SH bugs)\nthe extracted suffix from the bug. For all our experiments, we directly reuse the weights of each model. Our default setting for generation uses nucleus sampling [57] with top p = 0.95, temperature = 0.8 and 200 samples per bug. This generation setting is consistent with previous studies on LLMs [28], [31], [33]. Patches are generated on a 32-Core workstation with Ryzen Threadripper PRO 3975WX CPU, 256 GB RAM and NVIDIA RTX A6000 GPU, running Ubuntu 20.04.4 LTS.",
          "Below": [],
          "Grade": 2
        },
        {
          "Title": "Subject Systems",
          "Content": "\nFor evaluation, we use 5 APR benchmarks spanning across 3 programming languages. We focus on bugs where the fix is within a single function, which is also the focus of most recent APR work [21], [22], [24], [61]. To this end, we filter these benchmarks to find bugs that fit our designed repair settings. Table II presents the details of each repair dataset. Column Dataset is the dataset name, #Bugs is the total number of bugs, #SF, #SH, #SL shows the number of bugs which the reference fix is within a single function, single hunk (consecutive lines) and single line. Source refers to where the bugs are collected from, Language is the programming language of the bugs. We next discuss the detailed dataset information:\nDefects4J 1.2 and 2.0 [62]: The most widely studied APR benchmark with a collection of bugs gathered from open-source projects in Java containing pairs of buggy and patch versions of the source project. Since Defects4J has been updated to include more bugs from additional projects, we consider 2 different versions of Defects4J. Defects4J 1.2 contains 391 bugs (removing the 4 depreciated bugs) from 6 open-source Java projects. Defects4J 2.0 contains 438 new bugs from 9 additional projects. Each bug in Defects4J also contains developer tests exposing the bug.\nQuixBugs-Python and -Java [63]: A multi-lingual repair benchmark with 40 classic programming problems. QuixBugs benchmark is constructed from a programming challenge where programmers were asked to fix a small buggy function. QuixBugs was originally in Python but has been translated to Java, with both versions having the same 40 bugs. Each bug is accompanied with multiple test inputs and expected outputs.\nManyBugs [64]: A C repair dataset consisting of 185 bugs gathered from 9 open-source projects with developer written tests. Each bug is manually verified and classified into a bug type. However, we were not able to reproduce all bugs from the dataset (i.e., builds successfully and reference patches can pass all provided tests). As such we only use the 91 bugs where the results were reproducible by us.",
          "Below": [],
          "Grade": 2
        },
        {
          "Title": "Compared Techniques",
          "Content": "\nWe compare against the state-of-the-art APR baselines with both learning-based and traditional APR tools. We choose\n8 recent learning-based APR tools: AlphaRepair [26], Re- wardRepair [23], Recoder [21], DeepDebug [65], CURE [22],\nCoCoNuT [24], DLFix [66] and SequenceR [67]. Apart from AlphaRepair, these learning-based APR baselines are based on the NMT models. AlphaRepair combines a LLM (CodeBERT) with simple templates to generate patches under a zero- shot setting. Furthermore, we also choose 12 traditional APR tools: TBar [19], PraPR [20], AVATAR [16], SimFix [68],\nFixMiner [15], CapGen [9], JAID [69], SketchFix [13],\nNOPOL [12], jGenProg [70], jMutRepair [14], and jKali [14]. In total, we evaluate against 20 different APR tools. We compare against the baseline results on Defects4J 1.2, 2.0, QuixBugs-Python and Java on perfect fault localization - the ground-truth fix location is known to the repair tool. This is the preferred comparison setting as it eliminates the impact of differences in fault localization have on the result [21], [22], [24], [71]. Due to the lack of recent APR tools that are evaluated on ManyBugs, we only use it for RQ1. We follow prior work [19]\u2013[22] and directly use the correct patch results from previous studies [19], [20], [26].",
          "Below": [],
          "Grade": 2
        },
        {
          "Title": "Evaluation Metrics",
          "Content": "\nTo evaluate the repair performance, we use the standard metrics of plausible patches \u2013 passing the all test cases, and correct patches \u2013 syntactically or semantically equivalent to the reference patches. To determine correct patches, we follow the standard practice in APR research and manually inspect each plausible patch for semantic equivalency.",
          "Below": [],
          "Grade": 2
        }
      ],
      "Grade": 1
    },
    {
      "Title": "RESULT",
      "Content": "",
      "Below": [
        {
          "Title": "RQ1: Comparison of Different LLMs",
          "Content": "\nRepair effectiveness: We first compare LLMs against each other in generating plausible and correct patches. Ta- ble III shows the results of 6 generative models under complete function generation setting. The two integers in each cell represent the number of correct and plausible patches. We first observe that similar to previous studies in NLP [40], there is a scaling effect on the repair effectiveness. As we increase the size of the model, we also increase in the number\nTABLE V: Single line APR (SL bugs)\nDataset\tGPT-Neo\nGPT-Neo 1.3B\nGPT-Neo 2.7B\nGPT-J\tGPT-NeoX\tCodeT5\tINCODER\nInCoder\n6.7B\nCodex single-line\nCodex suffix\nof correct and plausible patches generated. Directly looking at the group of GPT models trained on the same dataset, we see that the performance consistently increases as we use larger models across all repair datasets. However, we see that the Codex model (12B) outperforms the biggest model (GPT- NeoX (20B)). We hypothesize that this is because Codex is designed and finetuned for code generation; on the other hand, while the training dataset of GPT-NeoX is partially made up of code, it is designed for general purpose text generation.\nTables IV and V show the results on the correct code infilling and single line generation repair tasks. Similar to the previous result, we again see the scaling effect of increased performance as model size increases. Compared to complete function generation, we observe that each model using correct code infilling and single line generation is able to produce a higher ratio of correct fixes to the total number of bugs. Furthermore, we also observe that the ratio of correct patches to plausible patches is higher in the latter 2 settings as well. This signals that patches produced using code infilling and single line generation is more likely to be the correct fix. The improved performance is because for complete function generation the model needs to understand the prompt given (Section III-B1), localize the bug and provide the correct fix. On the other hand, when we provide the model with the buggy location information in correct code infilling and single line generation, it only needs to fill in or complete the partial code, leading to more correct patches. This comparison is more direct when evaluating the Codex model, the only model that can perform both code infilling and function generation. We see that when performing correct code infilling, Codex is able to fix 40% (62/154) of the total bugs whereas when asked to generate the entire function, it drops to 28% (63/225).\nFor single line generation results in Table V, we included both generative and infilling models. However, for generative models we are not able to provide it with suffix code snippets since their generation is dependent only on the previous context. We compare this with infilling models, which can perform infilling conditioned on both the context before and after. We observe that infilling models perform better than their generative counterparts. Additionally, since we are able to use both the generative and infilling versions of Codex, we can directly compare the repair ability of the model when given only the prefix versus both prefix and suffix context. We see that when using the suffix information from the original buggy function, the Codex model is able to improve the number of correct and plausible fixes across all repair datasets. This shows that for repair, successfully utilizing the code after the buggy lines is important for fixing bugs.\nTABLE VI: Patch generation speed (#patch/min)\nModels\tDefects4J 1.2\tQuixBugs-Python\nFig. 4: Syntactic and semantic error rates on Defects4J 1.2\nSpeed: Next we look at the speed of patch generation using LLMs. We already saw from the previous result analysis that as we increase the size of the model, we obtain an increase in repair performance. However, such performance increase does not come for free as larger models require longer time for inferencing. Table VI shows the samples generated per minute for different LLMs on Defects4J 1.2 and QuixBugs- Python with the 3 repair generation settings (Columns CF, CI, SL refer to complete function, correct infilling and single line generation, respectively). We only include models that we run locally on the same hardware (i.e., excluding Codex since it is only accessible through API access). We first observe that as we increase model size, the patch generation speed drastically slows down (71x slower on GPT-NeoX than GPT-Neo 125M on complete function generation). This demonstrates the trade- off between repair effectiveness and time cost when using large models. Additionally, we see that compared to single line generation and correct code infilling, complete function generation takes significantly more time, since generating an entire function is much more time consuming than generating a single line or hunk. This shows while LLMs have the capability to perform fault localization and repair in one shot, for real- world software systems, it is still more cost-effective to first use traditional fault localization techniques [46] to pinpoint the precise bug locations and then leverage LLMs for more targeted patch generation.\nTABLE VII: Defects4J 1.2 baseline comparison\nOthers\nCombined\nCURE\n 36 \nRecoder\nAlphaRepair\nOthers\nCombined\nCURE\n 31 \nRecoder\nAlphaRepair\na) with all models\tb) with all models w/o exact developer patch\nFig. 5: Bug fix Venn diagram on Defects4J 1.2\nCompilation rate: We evaluate the compilation rate of the patches generated by each LLM. Figure 4 shows the syntactic and semantic error rates of all studied LLMs using the three repair settings on Defects4J 1.2. We first observe that the overall error rate (syntactic + semantic) of the generated patches goes down as we increase the size of the model. This reaffirms the previously discussed scaling effect of LLMs and show that the patches generated by larger models contain less errors. Next we see that all generative models using single line generation produced a high number of syntactic errors. Recall that single line generation when using generative models only provides the prefix in the buggy function. As a result, the generated line can easily introduce some syntax errors (e.g., adding an if statement with an opening bracket) since the model does not know what the suffix code context is. On the other hand, the amount of syntax errors produced in the two other settings are much lower. For complete function generation, LLMs can effectively retain the syntax of the language during training and generate syntactically correct functions. For correct code infilling, not only do we get low syntactic errors but also achieve the lowest semantic errors. Having both the prefix and suffix provides the model with sufficient context which leads to higher compilable patch rate.",
          "Below": [],
          "Grade": 2
        },
        {
          "Title": "RQ2: Comparison against State-of-the-art APR tools",
          "Content": "\nDefects4J 1.2 results: We first compare the results of directly using LLMs for repair against both traditional and learning-based APR tools on Defects4J 1.2. Table VII shows the number of correct bug fixes of the top baseline tools and also the LLMs in our evaluation. The last 3 columns present the number of correct patches generated when using each of the three APR settings. We then combine all patches generated for each of the models together (Column 2) to demonstrate the total number of fixes that can be obtained for the 255 single function bugs in Defects4J 1.2. Note that this is still a fair comparison \u2013 prior APR techniques typically use a timeout of 5h for each bug [21], [22], [26], while generating 200 patches for each of the 3 settings (i.e., at most 600 patches in total) costs no more than 2.5 hours for each model.\nWe observe that some of the models are able to achieve comparable performances compared to some of the recent\nBug: Mockito-22\nRepaired by: INCODER 6.7B\na)\nBug: Math-69 Repaired by: Codex\nb)\nFig. 6: Unique bug fixes generated by LLMs\nstate-of-the-art APR tools. Additionally, this result is obtained while generating only up to 600 samples per bug whereas prior approaches, especially learning-based tools, can generate up to 5000 patches per bug [22], [24], [26]. While the most effective model (Codex) can already outperform all existing techniques (e.g., fixing 99 single-function bugs), by combining the patches generated by all models (Total), we can achieve\n109 correct fixes on single function bugs! The surprising results show that by directly applying LLMs for APR without any specific change/finetuning, we can already achieve the highest number of correct fixes compared to existing baselines.\nFigure 5 presents the Venn diagram of unique fixes that can be generated using LLMs compared to the 3 best performing baselines on all the single function bugs in Defects4J 1.2. We also combine all fixes from other baselines together into the \u201cOthers\u201d category in the Venn diagram. We observe that by combining all the models together, we can generate a significant amount of unique bug fixes (36) that no other tools have fixed so far. Due to the potential data leakage issue (discussed in detail in Section VI), we further investigate whether LLMs can generate correct patches that are not exactly the same as developer patches. Figure 5b shows the unique bug fixes on Defects4J 1.2 compared to the baselines when we remove all fixes which are exactly the same as the developer patch. We observe that combining all LLMs together would still achieve the highest number of bug fixes (93) with 31 unique bug fixes.\nTo demonstrate the ability of these LLMs, we show some unique fixes produced by them. Figure 6a is a correct patch produced by the INCODER 6.7B model under correct code infilling task. We see here that the function is called areEqual and the bug is caused by missing a specific case of comparing\nTABLE VIII: Additional baseline comparison\nif the two inputs have the same reference. Using both the prefix (name of the function) and suffix (other comparison statements with return values), the model figures out the correct code to be inserted here (first checking if the references are the same before proceeding). Such code is commonly found in open-source projects which use similar comparison functions where the LLMs can learn from. In fact, we found several similar comparison functions (checking if the objects have the same reference) [72]\u2013[75] in different projects as a part of The Pile dataset [51] that some of the LLMs were trained on. Furthermore, unlike traditional APR tools which often work on a single line, LLMs can generate multiple lines of code in order to provide the correct fixes.\nFigure 6b shows a patch of the Math-69 bug generated by Codex. The function here calculates a matrix of p-values of a 2-sided, 2-sample t-test. The bug is caused by precision error when the function call is extremely close to 1. Here the model generates an alternative way of calculating the p-value which is much more stable than before. This is a hard bug to fix since the change is quite subtle but it does not fit any of the common templates used in traditional APR. To generate the correct fix, the model needs to understand the goal of the function (p- value calculation) and use statistical formulas. Both of which can be achieved by Codex as it is trained not only on code but also on general text, which contains many descriptions and examples of t-test p-value calculations. This unique fix shows the benefit of using LLMs for program repair where domain knowledge of the project can be utilized as well.\nAdditional results: In addition to comparing against state-of-the-art baselines on Defects4J 1.2, we also compare the performance of LLMs on other datasets widely used to evaluate previous APR tools. Table VIII shows the results on Defects4J 2.0, QuixBugs-Java and -Python where we also combine the correct bug fixes of the 3 generation strategies together. Similar to the Defects4J 1.2 results, we observe that many models can achieve similar (or even better) performance with carefully designed APR tools. More surprisingly, all 9 studied LLMs can outperform TBar, state-of-the-art template- based APR tool, and are competitive compared with the recent\nTABLE IX: Mean entropy of generated patches\nModels\t\tDefects4J 1.2\t\tQuixBugs-Python C\tP\tNP\t\tC\tP\tNP\nGPT-Neo 125M\t0.08\t0.13\t0.23\t0.10\t0.10\t0.20\nGPT-Neo 1.3B\t0.12\t0.12\t0.19\t0.06\t0.05\t0.09\nGPT-Neo 2.7B\t0.09\t0.13\t0.17\t0.05\t0.06\t0.08\nGPT-J\t0.07\t0.10\t0.12\t0.04\t0.05\t0.08\nGPT-NeoX\t0.08\t0.11\t0.13\t0.05\t0.07\t0.10\nCodex\t0.04\t0.05\t0.08\t0.11\t0.13\t0.16\nCodeT5\t0.50\t0.51\t0.54\t0.51\t0.50\t0.59\nINCODER 1.3B\t0.49\t0.58\t0.65\t0.54\t0.56\t0.65\nINCODER 6.7B\t0.45\t0.50\t0.61\t0.61\t0.60\t0.65\nCodex\t0.43\t0.43\t0.50\t0.32\t0.33\t0.42\nGPT-Neo 125M\t0.38\t0.42\t0.58\t0.41\t0.45\t0.61\nGPT-Neo 1.3B\t0.32\t0.38\t0.58\t0.25\t0.27\t0.47\nGPT-Neo 2.7B\t0.28\t0.32\t0.55\t0.21\t0.26\t0.40\nGPT-J\t0.29\t0.33\t0.54\t0.20\t0.22\t0.38\nGPT-NeoX\t0.39\t0.42\t0.71\t0.26\t0.28\t0.55\nCodex\t0.19\t0.28\t0.57\t0.18\t0.23\t0.60\nRecoder technique on the Defects4J 2.0 dataset. Furthermore, unlike many baselines which can only be used on a single language (specifically designed for a particular language or re- quiring additional finetuning on another language), the LLMs can be directly applied for multi-lingual repair.",
          "Below": [],
          "Grade": 2
        },
        {
          "Title": "RQ3: Patch Ranking and Correctness Checking Analysis",
          "Content": "\nEntropy: As we are using LLMs for patch generation, this allows us to compute the entropy of each patch. Entropy calculates how natural the generated sample is (Equation 1). Table IX shows the mean entropy values for correct (C), plausible (P) and non-plausible patches (NP). Each row shows the results of a LLM on a repair scenario containing bugs for which the LLM can produce a correct patch. We observe that average entropy value of correct and plausible patches for all models are less than non-plausible patches. Although not shown in the table, we observe the same finding when comparing patches using sum entropy. In other words, the studied LLMs consider correct patches which correctly fix the underlying bugs to be more nature than other patches. Additionally, while the entropy difference between correct and plausible patches is not as drastic as compared to non-plausible patches, we also find that correct patches are in general less entropic than plausible ones. Recent work [58] has shown that existing solutions for patch-correctness checking (i.e., identifying correct patches from plausible patches) can suffer from dataset overfitting and performance drops when applied on more complicated patches. We demonstrate for the first time that entropy computation via LLMs can help distinguish correct patches from plausible patches, indicating a promising future of directly leveraging the LLM entropy metric for patch- correctness checking.\nPatch ranking: Using the entropy values of each gen- erated patch, we perform ranking to validate patches with higher rank (lower entropy) first. We pick 5 LLMs with the highest number of correct patches to perform this analysis. Figure 7 shows the number of bugs fixed for the Defects4J 1.2 dataset using different patch ranking strategies as we increase the number of patches to validate. We see that compared to randomly picking patches to validate (blue line), when using\nFig. 7: Number of bugs fixes when using different patch ranking strategies on Defects4J 1.2\nTABLE X: Further improving LLM-based APR\nthe results when combining repair template with the IN-\nTools / Models\tDefects4J\nDefects4J 2.0 Single Line\nQuixBugs- Python\nCODER model. Following the AlphaRepair baseline, we apply different repair templates by using the original buggy line.\nAlphaRepair\t74\t35\t27\nRewardRepair\t50\t25\t-\nDeepDebug\t-\t-\t21\nRecoder\t65\t11\t-\nTBar\t68\t8\t-\nINCODER (200)\t37\t21\t27\nINCODER (2000)\t64\t25\t32\nINCODER w/ template (2000)\t78\t39\t37\nentropy rankings (orange and green line), we can validate the correct patches faster. This shows that entropy can be an effective measure used to rank the potential patches to prioritize lower entropy patches for validation under tighter time constraints. Furthermore, we observe that sum entropy performs slightly better compared to mean entropy. We hy- pothesize that this is because sum entropy calculates the entire sequence entropy regardless of the length of the generated sequence. As such, shorter sequences tend to have lower sum entropy compared to longer sequences; interestingly, this is consistent with traditional APR or patch correctness checking techniques [11], [76], [77], which favor simple patches over complicated ones following the Occam\u2019s razor hypothesis [78].",
          "Below": [],
          "Grade": 2
        },
        {
          "Title": "RQ4: Improvements on Direct LLM-based APR",
          "Content": "\nIn previous RQs, we showed that by directly applying LLMs for APR we can already achieve comparable performance with previous APR tools. We further explore the possibilities to boost the ability of LLMs for APR. For this experimental setup, we choose the best performing model (apart from Codex, which already outperforms existing APR techniques without any further extension) \u2013 INCODER 6.7B and run the model longer (2000 samples per bug) combined with repair templates. We evaluate on all bugs in Defects4J 1.2 by adjusting our infilling-style repair settings, following AlphaRe- pair [26] (which demonstrated the best performance among all settings in our study), to generate patches for every location which is changed by the reference patch instead of only on a single change location. This setup is similar to previous learning-based repair tools [21], [26] and allows us to compare on the full Defects4J 1.2 dataset. Furthermore, following prior work [26], we include evaluation on Defects4J 2.0 single line bugs and QuixBugs-Python.\nTable X shows the baseline tools along with our model setups. INCODER (200) is our default setup from previous evaluation that generates 200 samples per bug. INCODER (2000) shows the results when we increase the number of samples to 2000. INCODER w/ template (2000) contains\nSuch templates include: keeping parts of the prefix or suffix, replacing method calls or parameters, and changing/adding new boolean conditions or operators to the buggy line. These repair templates make use of the original buggy line and provide important starting code for the model.\nWe observe that if we apply the model longer and generate more samples, we can drastically improve the number of correct bugs fixed in all three datasets and achieve very close result to that obtained by the best baseline. Moreover, we can obtain further improvements by using simple repair templates and achieve the highest number of correctly fixed bugs on all datasets, e.g., fixing 78 bugs on Defects4J 1.2 with 15 unique bug fixes that no other baseline tools have fixed before. This finding shows that not only can LLMs be effective when directly used for program repair, we can combine them with more domain specific techniques such as simple repair templates to further improve their performance.",
          "Below": [],
          "Grade": 2
        }
      ],
      "Grade": 1
    },
    {
      "Title": "THREATS TO VALIDITY",
      "Content": "\nInternal. One internal threat to validity comes from our man- ual validation of plausible patches to determine semantically correct patches. To address this, we carefully performed the analysis and released the correct patches and code used to perform the experiments for public evaluation [79].\nAnother internal threat comes from the potential data leak- age of real developer patched functions being part of the original training data. To address this, we examine the patches LLMs generated for Defects4J 1.2 since this is the most widely studied dataset for APR and we mainly compared with state- of-the-art APR tools on this dataset. We first check if the bugs fixed by each LLM contain correct patches different than the reference developer patches. Out of the 354 individual bug fixes by all models on Defects4J 1.2, 234 fixes (66%) contain a patch that is different than the developer patch. We also found that due to the simplicity of single line patches, majority of the correct patches generated for single line bugs are the same as the developer patch. If we exclude single line bugs, the percentage increases to 77% (196/255). Out of the 109 bugs that can be fixed by combining all correct patches generated by all LLMs together (Total row in Table VII), 93 bugs (85%) are fixed by at least one correct patch that is different than the original developer patch, e.g., as shown in Figure 5b, removing LLM fixes that are exactly the same as the developer patches can still fix 31 bugs that prior tools cannot fix.\nSince we only have access to the training data used in CodeT5, GPT-Neo, GPT-J and GPT-NeoX models, we further check if the fixed function is within the training datasets when the correct patch is equivalent to the developer fix for these models. We found that while 38% (48/128) of bugs fixes contain only the same fix as the developer patch, only 15% (20/128) of those patches are also found in the original training data, showing that the majority of correct bug fixes provided by these LLMs are not simply from memorizing the training data. Moreover, our RQ4 shows that improvements can be further made by combining repair templates with LLMs, which is orthogonal to the data leakage issue. Additionally, We observe that LLMs are able to achieve the state-of-the-art results on QuixBugs dataset which is not part of the training data as it has low number of stars on GitHub and contains synthetic bugs and patches that are not part of any larger real-world projects. Further reducing the data leakage issue would require retraining the LLMs, which could be extremely costly.\nExternal. We evaluate LLMs on 5 repair datasets across 3 programming languages, making our evaluation one of the most comprehensive studies in APR. However, our findings may still not generalize to other datasets or languages.",
      "Below": [],
      "Grade": 1
    },
    {
      "Title": "DISCUSSION AND FUTURE WORK",
      "Content": "\nIn this work, we conduct a large-scale study on directly applying LLMs for APR, one of the most important problems in Software Engineering (SE). We demonstrate not only by directly applying LLMs we can already outperform prior APR techniques studied for over a decade, but also that we can further boost LLM performance by combining domain-specific techniques from SE. Building on these findings, we highlight two key directions for future work:\nImproving LLM performance for APR. We plan to con- tinue improving the performance of LLMs for APR. One approach is to use additional information, such as project- specific knowledge (i.e., from buggy project itself following the plastic surgery hypothesis [80]). For example, one could fine-tune/prompt the LLMs on the original buggy project to prime the model to generate code that fits the style/pattern used in the project. Another approach is to incorporate repair- specific knowledge by using additional templates as demon- strated in Section V-D to reduce the amount of code LLM has to generate and arrive at the correct patch faster. Along with these potential improvement directions, we also believe that we can use other new types of LLMs (i.e., dialogue- based) for APR such as ChatGPT [29]. ChatGPT is fine-tuned using reinforcement learning algorithm with human feedback designed for dialogues/conversations. We can leverage the currently underused testcase result to provide feedback to ChatGPT in a conversational manner, allowing the model to correct its previous mistakes and generate more correct patches [81].\nApplication of LLMs for other relevant SE tasks. While we study the performance of LLMs for APR, LLMs can be used for various other software engineering tasks. One such task is\nfuzzing [82], where LLMs can be potentially used to help gen- erate arbitrary inputs to fuzz test various software systems (in- cluding libraries, compilers, and interpreters). Compared with traditional automated fuzzing techniques [83] which require extensive human efforts for ensuring the syntactic/semantic va- lidity of input generation/mutation, LLMs offer an alternative solution by learning from billions of available code snippets in the wild to generate syntactically and also semantically valid input programs fully automatically (as demonstrated in recent work [84]). LLMs can also be used to target more context dependent tasks such as test [85] or test-oracle [86] generation. For example, while existing learning-based test- oracle generation techniques [87]\u2013[89] mainly formulate the problem as a classification or NMT problem, another natural solution could be to leverage the LLMs to directly complete or infill the oracles based on context information (such as focal method and test prefix/suffix). Similar to APR, mutation testing [90] or bug seeding in general [89], [91], [92] also applies systematic modifications to programs under test. As a result, it is also very natural to directly apply infilling-style APR techniques (such as AlphaRepair [26]) for such domains. In addition to these discussed SE tasks above, we believe our study results and techniques can also motivate, inspire, and be applied to many other relevant SE tasks involving code generation/mutation. These potential applications along with LLMs for APR highlight the promising future of using LLMs to help with SE in general.",
      "Below": [],
      "Grade": 1
    },
    {
      "Title": "CONCLUSION",
      "Content": "\nWe present an extensive evaluation on LLMs for automated program repair. We use 9 state-of-the-art LLMs with 5 differ- ent repair datasets and design different practical repair settings to compare and contrast the repair effectiveness of different LLMs. In our evaluation, we shed light on the scaling effect that increasing model size has on various important factors in APR such as the number of bugs fixed, the speed of patch generation, and the compilation rate. Also, we compare the performance of LLMs against state-of-the-art APR tools and highlight the unique fixes and advantages of using LLMs for APR. Furthermore, we evaluated the ability for LLMs to perform patch ranking and patch correctness checking in order to prioritize correct patches for faster repair. Lastly, we demonstrate the possibilities (i.e., increasing the sample size and combining LLMs with repair templates) to further boost the performance of LLMs for APR. The results from our study demonstrate promising future of adopting LLMs for APR and beyond (e.g., other SE tasks involving program generation/mutation).\nAcknowledgments\nWe thank the reviewers for their insightful feedback and comments to improve this paper. We thank Yifeng Ding for his helpful discussion on this work. This work was partially supported by NSF grants CCF-2131943, and CCF-2141474. We also acknowledge support from Kwai Inc. and Ant Group.",
      "Below": [],
      "Grade": 1
    },
    {
      "Title": "REFERENCES",
      "Content": "\nK. Luzniak, \u201cSoftware for the healthcare industry: what is it and why it\u2019s worth using?\u201d neoteric, 2022, https://neoteric.eu/blog/ software-for-the-healthcare-industry-what-is-it-and-why-its-worth-using.\nN. Mayersohn, \u201cData driving new approaches to transportation,\u201d The New York Times, 2022, https://www.nytimes.com/2020/02/05/ technology/data-micromobility-electric-scooters-mds.html.\nE. Richards, \u201cSoftware\u2019s dangerous aspect,\u201d The Washington Post, 1990, https://www.washingtonpost.com/archive/politics/1990/12/09/ softwares-dangerous-aspect/9b2e9243-8deb-4ac7-9e8f-968de0806e5e/.\nS.   Matteson,   \u201cReport:   Software   failure   caused\n$1.7  trillion  in  financial  losses  in  2017,\u201d  TechRe- public,\t2018,\thttps://www.techrepublic.com/article/ report-software-failure-caused-1-7-trillion-in-financial-losses-in-2017/.\nD. H. O\u2019Dell, \u201cThe debugging mindset,\u201d acmqueue, 2017, https://queue. acm.org/detail.cfm?id=3068754/.\nL. Gazzola, D. Micucci, and L. Mariani, \u201cAutomatic software repair: A survey,\u201d IEEE Transactions on Software Engineering, vol. 45, 2019.\nC. Le Goues, T. Nguyen, S. Forrest, and W. Weimer, \u201cGenprog: A generic method for automatic software repair,\u201d IEEE Transactions on Software Engineering, vol. 38, 2012.\nX. B. D. Le, D. Lo, and C. Le Goues, \u201cHistory driven program repair,\u201d in SANER, 2016.\nM. Wen, J. Chen, R. Wu, D. Hao, and S.-C. Cheung, \u201cContext-aware patch generation for better automated program repair,\u201d in ICSE, 2018.\nS. Mechtaev, J. Yi, and A. Roychoudhury, \u201cAngelix: Scalable multiline program patch synthesis via symbolic analysis,\u201d in ICSE, 2016.\nX.-B. D. Le, D.-H. Chu, D. Lo, C. Le Goues, and W. Visser, \u201cS3: syntax- and semantic-guided repair synthesis via programming by examples,\u201d in ESEC/FSE, 2017.\nF. DeMarco, J. Xuan, D. Le Berre, and M. Monperrus, \u201cAutomatic repair of buggy if conditions and missing preconditions with smt,\u201d in Proceedings of the 6th International Workshop on Constraints in Software Testing, Verification, and Analysis, 2014.\nJ. Hua, M. Zhang, K. Wang, and S. Khurshid, \u201cSketchfix: A tool for automated program repair approach using lazy candidate generation,\u201d in ESEC/FSE, 2018.\nM. Martinez and M. Monperrus, \u201cAstor: A program repair library for java (demo),\u201d in ISSTA, 2016.\nA. Koyuncu, K. Liu, T. F. Bissyande\u00b4, D. Kim, J. Klein, M. Monperrus, and Y. L. Traon, \u201cFixminer: Mining relevant fix patterns for automated program repair,\u201d Empir. Softw. Eng., vol. 25, 2020.\nK. Liu, A. Koyuncu, D. Kim, and T. F. Bissyande\u00b4, \u201cAVATAR: fixing semantic bugs with fix patterns of static analysis violations,\u201d in Proceed- ings of the 26th IEEE International Conference on Software Analysis, Evolution, and Reengineering, 2019.\nY. Lou, A. Ghanbari, X. Li, L. Zhang, H. Zhang, D. Hao, and L. Zhang, \u201cCan automated program repair refine fault localization? a unified debugging approach,\u201d in ISSTA, 2020.\nS. Benton, X. Li, Y. Lou, and L. Zhang, \u201cOn the effectiveness of unified debugging: An extensive study on 16 program repair systems,\u201d in ASE, 2020.\nK. Liu, A. Koyuncu, D. Kim, and T. F. Bissyande\u00b4, \u201cTbar: Revisiting template-based automated program repair,\u201d in ISSTA, 2019.\nA. Ghanbari, S. Benton, and L. Zhang, \u201cPractical program repair via bytecode mutation,\u201d in ISSTA, 2019.\nQ. Zhu, Z. Sun, Y.-a. Xiao, W. Zhang, K. Yuan, Y. Xiong, and L. Zhang, \u201cA syntax-guided edit decoder for neural program repair,\u201d in ESEC/FSE, 2021.\nN. Jiang, T. Lutellier, and L. Tan, \u201cCure: Code-aware neural machine translation for automatic program repair,\u201d ICSE, 2021.\nH. Ye, M. Martinez, and M. Monperrus, \u201cNeural program repair with execution-based backpropagation,\u201d in ICSE, 2022.\nT. Lutellier, H. V. Pham, L. Pang, Y. Li, M. Wei, and L. Tan, \u201cCoconut: Combining context-aware neural translation models using ensemble for program repair,\u201d in ISSTA, 2020.\nI. Sutskever, O. Vinyals, and Q. V. Le, \u201cSequence to sequence learning with neural networks,\u201d 2014, arXiv:1409.3215.\nC. S. Xia and L. Zhang, \u201cLess training, more repairing please: Revisiting automated program repair via zero-shot learning,\u201d in ESEC/FSE, 2022.\nT. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhari- wal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal,\nA. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M.\nZiegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin,\nS. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford,\nI. Sutskever, and D. Amodei, \u201cLanguage models are few-shot learners,\u201d 2020, arXiv:2005.14165.\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan,\nH. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri,\nG. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan,\nS. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian,\nC. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis,\nE. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak,\nJ. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse,\nA. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford,\nM. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew,\nD. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba, \u201cEvaluating large language models trained on code,\u201d 2021, arXiv:2107.03374.\nJ. Schulman, B. Zoph, J. H. Christina Kim, J. Menick, J. Weng,\nJ. F. C. Uribe, L. Fedus, L. Metz, M. Pokorny, R. G. Lopes, S. Zhao,\nA. Vijayvergiya, E. Sigler, A. Perelman, C. Voss, M. Heaton, J. Parish,\nD. Cummings, R. Nayak, V. Balcom, D. Schnurr, T. Kaftan, C. Hallacy,\nN. Turley, N. Deutsch, V. Goel, J. Ward, A. Konstantinidis, W. Zaremba,\nL. Ouyang, L. Bogdonoff, J. Gross, D. Medina, S. Yoo, T. Lee, R. Lowe,\nD. Mossing, J. Huizinga, R. Jiang, C. Wainwright, D. Almeida, S. Lin,\nM. Zhang, K. Xiao, K. Slama, S. Bills, A. Gray, J. Leike, J. Pachocki,\nP. Tillet, S. Jain, G. Brockman, and N. Ryder, \u201cChatgpt: Optimizing language models for dialogue,\u201d 2022, https://openai.com/blog/chatgpt/.\nZ. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin,\nT. Liu, D. Jiang, and M. Zhou, \u201cCodebert: A pre-trained model for programming and natural languages,\u201d 2020, arXiv:2002.08155.\nS. D. Kolak, R. Martins, C. L. Goues, and V. J. Hellendoorn, \u201cPatch generation with language models: Feasibility and scaling behavior,\u201d in Deep Learning for Code Workshop, 2022.\nJ. A. Prenner, H. Babii, and R. Robbes, \u201cCan openai\u2019s codex fix bugs?: An evaluation on quixbugs,\u201d in 2022 IEEE/ACM International Workshop on Automated Program Repair (APR), 2022.\nD. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi, R. Zhong, W.-t. Yih, L. Zettlemoyer, and M. Lewis, \u201cIncoder: A generative model for code infilling and synthesis,\u201d 2022, arXiv:2204.05999.\nS. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco,\nC. Clement, D. Drain, D. Jiang, D. Tang et al., \u201cCodexglue: A machine learning benchmark dataset for code understanding and generation,\u201d 2021, arXiv:2102.04664.\nK. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, \u201cBleu: A method for automatic evaluation of machine translation,\u201d in Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, 2002.\nY. Liu, \u201cFine-tune bert for extractive summarization,\u201d 2019, arXiv:1903.10318.\nZ. Yang, Z. Dai, Y. Yang, J. Carbonell, R. Salakhutdinov, and Q. V. Le, \u201cXlnet: Generalized autoregressive pretraining for language understand- ing,\u201d 2020, arXiv:1906.08237.\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d 2017, arXiv:1706.03762.\nL. Reynolds and K. McDonell, \u201cPrompt programming for large language models: Beyond the few-shot paradigm,\u201d 2021, arXiv:2102.07350.\nJ. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,\nS. Gray, A. Radford, J. Wu, and D. Amodei, \u201cScaling laws for neural language models,\u201d 2020, arXiv:2001.08361.\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre-training of deep bidirectional transformers for language understanding,\u201d 2018, arXiv:1810.04805.\nS. Black, L. Gao, P. Wang, C. Leahy, and S. Biderman, \u201cGPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow,\u201d\nMar. 2021. [Online]. Available: https://doi.org/10.5281/zenodo.5297715\nC. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\nY. Zhou, W. Li, and P. J. Liu, \u201cExploring the limits of transfer learning with a unified text-to-text transformer,\u201d J. Mach. Learn. Res., 2020.\nM. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy,\nV. Stoyanov, and L. Zettlemoyer, \u201cBart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehen- sion,\u201d 2019, arXiv:1910.13461.\nA. Aghajanyan, B. Huang, C. Ross, V. Karpukhin, H. Xu, N. Goyal,\nD. Okhonko, M. Joshi, G. Ghosh, M. Lewis, and L. Zettlemoyer, \u201cCm3: A causal masked multimodal model of the internet,\u201d 2022, arXiv:2201.07520.\nR. Abreu, P. Zoeteweij, and A. J. van Gemund, \u201cOn the accuracy of spectrum-based fault localization,\u201d in Testing: Academic and In- dustrial Conference Practice and Research Techniques - MUTATION (TAICPART-MUTATION 2007), 2007.\nA. Hindle, E. T. Barr, Z. Su, M. Gabel, and P. Devanbu, \u201cOn the naturalness of software,\u201d in ICSE, 2012.\n\u201cHugging face,\u201d 2022, https://huggingface.co.\nB. Wang and A. Komatsuzaki, \u201cGPT-J-6B: A 6 Billion Param- eter Autoregressive Language Model,\u201d https://github.com/kingoflolz/ mesh-transformer-jax, May 2021.\nS. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Golding,\nH. He, C. Leahy, K. McDonell, J. Phang, M. Pieler, U. S. Prashanth,\nS. Purohit, L. Reynolds, J. Tow, B. Wang, and S. Weinbach, \u201cGPT-NeoX- 20B: An open-source autoregressive language model,\u201d in Proceedings of the ACL Workshop on Challenges & Perspectives in Creating Large Language Models, 2022, arXiv:2204.06745.\nL. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster,\nJ. Phang, H. He, A. Thite, N. Nabeshima et al., \u201cThe pile: An 800gb dataset of diverse text for language modeling,\u201d 2020, arXiv:2101.00027.\nS. J. Yue Wang, Weishi Wang and S. C. Hoi, \u201cCodet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation,\u201d in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, 2021.\nH. Husain, H.-H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt, \u201cCodesearchnet challenge: Evaluating the state of semantic code search,\u201d 2020, arXiv:1909.09436.\n\u201cBigquery github repos,\u201d 2022, https://console.cloud.google.com/ marketplace/details/github/github-repos.\n\u201cCodex suffix api,\u201d https://beta.openai.com/docs/api-reference/ completions/create#completions/create-suffix, 2022.\nL. Zhang, L. Zhang, and S. Khurshid, \u201cInjecting mechanical faults to localize developer faults for evolving software,\u201d ACM SIGPLAN Notices, vol. 48, 2013.\nA. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi, \u201cThe curious case of neural text degeneration,\u201d 2019, arXiv:1904.09751.\nY. Wang, J. Yang, Y. Lou, M. Wen, and L. Zhang, \u201cAtten- tion: Not just another dataset for patch-correctness checking,\u201d 2022, arXiv:2207.06590.\n\u201cPytorch,\u201d 2022, http://pytorch.org.\n\u201cOpenai api,\u201d 2022, https://openai.com/api.\nY. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond,\nT. Eccles, J. Keeling, F. Gimeno, A. D. Lago, T. Hubert, P. Choy,\nC. d. M. d\u2019Autume, I. Babuschkin, X. Chen, P.-S. Huang, J. Welbl,\nS. Gowal, A. Cherepanov, J. Molloy, D. J. Mankowitz, E. S. Robson,\nP. Kohli, N. de Freitas, K. Kavukcuoglu, and O. Vinyals, \u201cCompetition- level code generation with alphacode,\u201d 2022, arXiv:2203.07814.\nR. Just, D. Jalali, and M. D. Ernst, \u201cDefects4j: A database of existing faults to enable controlled testing studies for java programs,\u201d ser. ISSTA, 2014.\nD. Lin, J. Koppel, A. Chen, and A. Solar-Lezama, \u201cQuixbugs: A multi- lingual program repair benchmark set based on the quixey challenge,\u201d ser. SPLASH Companion 2017, 2017.\nC. Le Goues, N. Holtschulte, E. K. Smith, Y. Brun, P. Devanbu,\nS. Forrest, and W. Weimer, \u201cThe manybugs and introclass benchmarks for automated repair of c programs,\u201d IEEE Transactions on Software Engineering, vol. 41, 2015.\nD. Drain, C. B. Clement, G. Serrato, and N. Sundaresan, \u201cDeepdebug: Fixing python bugs using stack traces, backtranslation, and code skele- tons,\u201d 2021, arXiv:2105.09352.\nY. Li, S. Wang, and T. N. Nguyen, \u201cDlfix: Context-based code transfor- mation learning for automated program repair,\u201d in ICSE, 2020.\nZ. Chen, S. Kommrusch, M. Tufano, L.-N. Pouchet, D. Poshyvanyk, and\nM. Monperrus, \u201cSequencer: Sequence-to-sequence learning for end-to- end program repair,\u201d IEEE Transaction on Software Engineering, 2019.\nJ. Jiang, Y. Xiong, H. Zhang, Q. Gao, and X. Chen, \u201cShaping program repair space with existing patches and similar code,\u201d in ISSTA, 2018.\nL. Chen, Y. Pei, and C. A. Furia, \u201cContract-based program repair without the contracts,\u201d in ASE, 2017.\nM. Martinez, T. Durieux, J. Xuan, R. Sommerard, and M. Monperrus, \u201cAutomatic repair of real bugs: An experience report on the defects4j dataset,\u201d 2015, arXiv:1505.07002.\nM. Tufano, C. Watson, G. Bavota, M. Di Penta, M. White, and\nD. Poshyvanyk, \u201cAn empirical investigation into learning bug-fixing patches in the wild via neural machine translation,\u201d in ASE, 2018.\n\u201cjetbrick-template-2x\tobject\tcomparison\tcode,\u201d\t2022, https://github.com/subchen/jetbrick-template-2x/blob/ def3107e2878aa5bee32ac2ba3be8e241fba4a64/src/main/java/jetbrick/ template/parser/ast/ALU.java#L421-L448.\n\u201cgoclipse object comparison code,\u201d 2022, https://github.com/GoClipse/ goclipse/blob/e135d3a69e6498e278521c2542cee3808bd1377d/plugin tooling/src-util/melnorme/utilbox/core/CoreUtil.java#L28-L30.\n\u201cteiid object comparison code,\u201d 2022, https://github.com/teiid/teiid/blob/ 21c93a6fd4be2528f95224f99905d74479862d1b/federate-common-core/ src/main/java/com/metamatrix/core/util/EquivalenceUtil.java#L49-L57.\n\u201cGroza\tobject\tcomparison\tcode,\u201d\t2022, https://github.com/IoT-Technology/Groza/blob/ fbafceef53d646025046990ffbd89bf701c56b45/dao/src/main/java/ com/sanshengshui/server/dao/util/mapping/JsonTypeDescriptor.java# L49-L58.\nM. Asad, K. K. Ganguly, and K. Sakib, \u201cImpact analysis of syntactic and semantic similarities on patch prioritization in automated program repair,\u201d in ICSME, 2019.\nQ. Xin and S. P. Reiss, \u201cLeveraging syntax-related code for automated program repair,\u201d in ASE, 2017.\nE. Sober, Ockham\u2019s razors. Cambridge University Press, 2015.\n\u201cDataset,\u201d 2023, https://zenodo.org/record/7592886.\nE. T. Barr, Y. Brun, P. Devanbu, M. Harman, and F. Sarro, \u201cThe plastic surgery hypothesis,\u201d in ESEC/FSE, 2014.\nC. S. Xia and L. Zhang, \u201cConversational automated program repair,\u201d 2023, arXiv:2301.13246.\nA. Zeller, R. Gopinath, M. Bo\u00a8hme, G. Fraser, and C. Holler, \u201cThe fuzzing book,\u201d 2019.\nZ. Manna and R. J. Waldinger, \u201cToward automatic program synthesis,\u201d\nCommun. ACM, vol. 14, no. 3, p. 151\u2013165, mar 1971.\nY. Deng, C. S. Xia, H. Peng, C. Yang, and L. Zhang, \u201cFuzzing deep- learning libraries via large language models,\u201d 2022, arXiv:2212.14834.\nG. Fraser and A. Arcuri, \u201cWhole test suite generation,\u201d IEEE Transac- tions on Software Engineering, vol. 39, 2012.\nM. D. Ernst, J. H. Perkins, P. J. Guo, S. McCamant, C. Pacheco, M. S. Tschantz, and C. Xiao, \u201cThe daikon system for dynamic detection of likely invariants,\u201d Science of computer programming, vol. 69, no. 1-3,\npp. 35\u201345, 2007.\nE. Dinella, G. Ryan, T. Mytkowicz, and S. K. Lahiri, \u201cToga: a neural method for test oracle generation,\u201d in ICSE, 2022.\nC. Watson, M. Tufano, K. Moran, G. Bavota, and D. Poshyvanyk, \u201cOn learning meaningful assert statements for unit test cases,\u201d in ICSE, 2020.\nD. B. Brown, M. Vaughn, B. Liblit, and T. Reps, \u201cThe care and feeding of wild-caught mutants,\u201d in Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering, 2017, pp. 511\u2013522.\nY. Jia and M. Harman, \u201cAn analysis and survey of the development of mutation testing,\u201d IEEE transactions on software engineering, vol. 37, 2010.\nJ. Patra and M. Pradel, \u201cSemantic bug seeding: a learning-based ap- proach for creating realistic bugs,\u201d in Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 2021, pp. 906\u2013918.\nZ. Tian, J. Chen, Q. Zhu, J. Yang, and L. Zhang, \u201cLearning to construct better mutation faults,\u201d in 37th IEEE/ACM International Conference on Automated Software Engineering, 2022, pp. 1\u201313.",
      "Below": [],
      "Grade": 1
    }
  ]
}